#!/usr/bin/env python
# prepare the data for the mutation finder
# e.g. uniprot sequences, entrez mapping, etc

# load default python packages
import sys, logging, optparse, os, glob, shutil, gzip, collections, marshal, gdbm, re, zlib, cPickle
import struct
import urllib, urllib2
import xml.etree.ElementTree as et
from os.path import *
from collections import defaultdict

# add <scriptDir>/lib/ to package search path
progFile = os.path.abspath(sys.argv[0])
progDir  = os.path.dirname(progFile)
pubToolsLibDir = os.path.join(progDir, "lib")
sys.path.insert(0, pubToolsLibDir)

# now load our own libraries
import pubConf, pubGeneric, util, maxbio, maxCommon, pslMapBed
from maxCommon import runCommand, makeOrCleanDir
from os.path import *
from pycbio.hgdata import Psl

# === COMMAND LINE INTERFACE, OPTIONS AND HELP ===
parser = optparse.OptionParser("""usage: %prog entrez|other|all: reformat uniprot sequences, entrez data etc for mutation finder and write to tools/data/mutFinder
""")

parser.add_option("-d", "--debug", dest="debug", action="store_true", help="show debug messages") 
parser.add_option("-v", "--verbose", dest="verbose", action="store_true", help="show more debug messages")
(options, args) = parser.parse_args()

# ==== FUNCTIONS =====
taxToDb = {9606 : "hg19"}
mutDataDir = None

def writeGzDict(data, fname):
    logging.info("Writing %d keys to %s" % (len(data), fname))
    ofh = gzip.open(fname, "w")
    for key, valList in data.iteritems():
        for val in valList:
            assert("|" not in val)
        ofh.write("%s\t%s\n" % (key, "|".join(valList)))

def parseUniprot(uniprotDir, taxId):
    """ parse uniprot tab and return as three dicts 
        entrezId -> upId
        upId -> symbol
        upId -> list of gb accessions
        upId -> list of refseq IDs
    """
    tabFname = join(uniprotDir, "uniprot.tab")
    logging.info("Parsing entrez <-> uniprot from %s" % tabFname)
    entrezToUp = {}
    upToSym = {}
    upToGb = {}
    upToRefseq = defaultdict(list)

    entrezCount = 0
    duplCount = 0
    for row in maxCommon.iterTsvRows(tabFname):
        if int(row.taxonId)==taxId:
            if row.geneName!="":
                upToSym[row.acc] = row.geneName
            if row.refSeq!="":
                refseqIds = row.refSeq.split("|")
                upToRefseq[row.acc]=refseqIds
            if row.ncbiGene!="":
                ncbiGenes = row.ncbiGene.split("|")
                for ncbiGene in ncbiGenes:
                    ncbiGene = int(ncbiGene)
                    entrezCount += 1
                    if ncbiGene in entrezToUp:
                        duplCount +=1
                    entrezToUp[ncbiGene] = row.acc
            if row.embl!="":
                emblIds = row.embl.split("|")
                upToGb[row.acc] = emblIds
    logging.info("%d entrez -> uniprot links, skipped duplicates: %d" % (entrezCount, duplCount))
    return entrezToUp, upToSym, upToGb, upToRefseq

def parseEntrezGeneRefseq(taxId, outFname):
    " create a tab-sep file with entrezGene, comma sep refseqIds, comma sep refseqProtIds"
    fname = join(pubConf.ncbiGenesDir, "gene2refseq.gz")
    logging.info("Parsing %s" % fname)
    # parse refseq into dicts
    refseqs = {}
    refprots = {}
    refsym = {}
    for line in gzip.open(fname):
       if not line.startswith("9606"):
           continue
       fs = line.strip("\n").split("\t")
       if not fs[0]=="9606":
           continue
       #print fs
       #if len(fs)<7:
            # some genes have no refseq info
            #continue
       tax, geneId, desc, refseqId, gir, refProtId, gip = fs[:7]
       sym = fs[15]
       if desc=="SUPPRESSED":
           continue
       if sym!="-":
           refsym[int(geneId)] = sym
       if refseqId!="-":
           refseqs.setdefault(int(geneId), set()).add(refseqId)
       if refProtId!="-":
           refprots.setdefault(int(geneId), set()).add(refProtId)

    # output dicts to tab sep file 
    logging.info("tab output...")
    ofh = open(outFname, "w")
    ofh.write("\t".join(["entrezId", "sym", "refseqIds", "refseqProtIds"]))
    ofh.write("\n")
    for geneId, refseqIds in refseqs.iteritems():
        refseqProtIds = refprots.get(geneId, [])
        sym = refsym.get(geneId, "")
        row = [str(geneId), sym, ",".join(refseqIds), ",".join(refseqProtIds)]
        ofh.write("\t".join(row))
        ofh.write("\n")
    ofh.close()
    logging.info("Wrote %s" % outFname)

    # write to marshal file
    outFname += ".marshal"
    data = {}
    data["entrez2refseqs"]  = refseqs
    data["entrez2refprots"] = refprots
    data["entrez2sym"] = refsym
    marshal.dump(data, open(outFname, "w"))
    logging.info("Wrote %s" % outFname)
    
def parseEntrezGenePmids(taxId, dbm):
    pmid2geneFname = join(pubConf.ncbiGenesDir, "gene2pubmed.gz")
    # at ucsc: /hive/data/outside/ncbi/genes/gene2pubmed.gz
    logging.info("Parsing %s" % pmid2geneFname)
    pmidToEntrez = {}
    for line in gzip.open(pmid2geneFname):
        if line.startswith("#"):
            continue
        row = line.rstrip("\n").split("\t")
        rowTax, entrezId, pmid = row
        if int(rowTax)==int(taxId):
            pmidToEntrez.setdefault(int(pmid), []).append(entrezId)
    logging.info("Taxon %d: found entrez ids for %s pmids" % (int(taxId), len(pmidToEntrez)))

    logging.info("Writing to dbm file")
    for pmid, entrezList in pmidToEntrez.iteritems():
        dbm[str(pmid)] = ",".join(entrezList)

def faToDbm(faName, dbm):
    logging.info("indexing %s into dbm as compressed seqs" % faName)
    faSizeOfh = open(faName+".size", "w")
    #logging.info("Reading %s" % faName)
    for seqId, seq in maxbio.parseFasta(faName):
        dbm[seqId] = zlib.compress(seq)
    logging.info("Converted %s to dbm" % (faName))

def tabToDbm(fname):
    dbmFname = fname+".dbm"
    logging.info("Indexing %s to %s" % (fname, dbmFname))
    dbm = gdbm.open(dbmFname, "nf")
    for row in maxCommon.iterTsvRows(fname):
        key, val = row
        dbm[key] = val
    dbm.close()

def parseRa(raName, tabName):
    " write refseqId.version, refProt ID and cds Start to a tabular file "
    logging.info("Parsing ra")
    ofh = open(tabName, "w")
    ofh.write("refSeq\trefProt\tcdsStart\n")
    id = None
    cds = None
    prt = None
    data = {}
    skipRec = False
    skipCount = 0
    accList = []
    for line in open(raName):
        if line.startswith("acc"):
            id = line.rstrip("\n").split()[1]
            continue
        if line.startswith("ver"):
            ver = line.rstrip("\n").split()[1]
            continue
        if line.startswith("prt"):
            prt = line.rstrip("\n").split()[1]
            continue
        if line.startswith("cds"):
            cds = line.rstrip("\n").split()[1].split(".")[0]
            if "join" in line:
                skipRec = True
            continue
        if line=="\n" and id!=None and cds!=None and prt!=None:
            if skipRec:
                skipCount += 1
            else:
                acc = id+"."+ver
                row = [acc, prt, cds]
                ofh.write("\t".join(row))
                ofh.write("\n")
                accList.append(acc)
            id = None
            cds = None
            prt = None
            skipRec = False
    ofh.close()
    logging.info("Wrote cds and pep/refseq assignment to %s" % tabName)
    logging.info("Skipped %d records" % skipCount)
    return accList

def makeOldAccs(newAccList):
    " given a list of things like NM_000325.5, return NM_000325.4, NM_000325.3, etc. "
    oldAccs = []
    for fullAcc in newAccList:
        acc, ver = fullAcc.split(".")
        ver = int(ver)
        if ver != 1:
            for i in range(1, ver):
                oldAccs.append(acc+"."+str(i))
    return oldAccs

def chunkedDownloadFromEutils(accs, outFh, format="fasta"):
    " download in chunks of 5000 accs from eutils "
    chunkMax = 5000
    for chunkStart in range(0, len(accs), chunkMax):
        logging.info("Chunkstart is %d" % chunkStart)
        downloadFromEutils(accs[chunkStart:chunkStart+chunkMax], outFh, format="fasta")

def downloadFromEutils(accs, outFh, format="fasta"):
    " download accession via eutils "
    # query and put into history
    logging.info("Running eutils search for %d accessions" % len(accs))
    accFull = [acc for acc in accs]
    query = " OR ".join(accFull)
    base = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    url = base+'/esearch.fcgi';
    data = urllib.urlencode({"db":"nucleotide", "term":query, "usehistory":"y"})
    logging.debug("Data: %s" % data)
    req  = urllib2.Request(url, data)
    try:
        xmlStr = urllib2.urlopen(req).read()
    except urllib2.HTTPError:
        logging.error("Error when searching")
        raise
    logging.debug("XML reply: %s" % xmlStr)

    # now parse xml to get history IDs
    root = et.fromstring(xmlStr)
    try:
        webEnv = root.find("WebEnv").text
        key = root.find("QueryKey").text
    except AttributeError:
        logging.info("Error, XML is %s" % xmlStr)
        raise
    count = int(root.find("Count").text)
    logging.info("Got history id %s, found %d sequences" % (webEnv, count))

    # fetch
    retMax = 1000
    for retStart in range(0, count, retMax):
        logging.info("Retrieving up to %d records, start %d..." % (retMax, retStart))
        url = base + "/efetch.fcgi?db=nuccore&query_key=%(key)s&WebEnv=%(webEnv)s&" \
            "retstart=%(retStart)s&retmax=%(retMax)s&rettype=%(format)s&retmode=text" % locals();
        resp = urllib2.urlopen(url).read()
        outFh.write(resp)

def parseRefseq(taxId, mutDataDir):
    """ get prot <-> trans assignment and cdsStart """
    raName =  join(mutDataDir, "refseq.%s.ra"  % str(taxId))
    logging.info("Getting ra to %s" % raName)
    cmd = "gbGetSeqs -gbRoot=/hive/data/outside/genbank RefSeq mrna %s -get=ra -db=hg19 -inclVersion -native" % raName
    maxCommon.runCommand(cmd)
    refseqInfoFname =  join(mutDataDir, "refseqInfo.tab")
    accList = parseRa(raName, refseqInfoFname)
    os.remove(raName)

def loadSeqs(taxId, mutDataDir, uniprotDir):
    """ parse refseq and uniprot sequences as compressed values to gdbm file """
    assert(taxId==9606)

    # get old refseqs, too
    #oldRefseqFname = join(mutDataDir, "oldRefseq.%s.gb")
    #logging.info("Downloading old refseqs to %s" % oldRefseqFname)
    #oldAccs = makeOldAccs(accList)
    #outFh = open(oldRefseqFname, "w")
    #chunkedDownloadFromEutils(oldAccs, outFh)
    #outFh.write("\n".join(oldAccs[:1000]))
    #assert(False)

    # get fastas
    cmdTemp = "gbGetSeqs -gbRoot=/hive/data/outside/genbank RefSeq %s %s -db=hg19 -inclVersion"
    transFaName = join(mutDataDir, "refseq.%s.trans.fa" % str(taxId))
    protFaName =  join(mutDataDir, "refseq.%s.prot.fa"  % str(taxId))
    for seqType, fname in [("mrna", transFaName), ("pep", protFaName)]:
        logging.info("Getting data for %s" % seqType)
        cmd = cmdTemp % (seqType, fname)
        maxCommon.runCommand(cmd)
    logging.info("Wrote fastas to %s and %s" % (transFaName, protFaName))

    # index fastas
    dbmTmpFname = join(pubConf.getFastTempDir(), "seqs.dbm")
    upFaName = join(uniprotDir, "uniprot.%d.fa.gz" % taxId)

    dbm = gdbm.open(dbmTmpFname, "nf")
    faToDbm(upFaName, dbm)
    faToDbm(transFaName, dbm)
    faToDbm(protFaName, dbm)
    dbm.close()

    dbmFname = join(mutDataDir, "seqs.dbm")
    shutil.copy(dbmTmpFname, dbmFname)
    os.remove(dbmTmpFname)
    logging.info("Wrote compressed refseq seqs to %s" % dbmFname)

def writeUpRefseq(taxId, uniprotDir, pairFname):
    " write a list of tuples refseqId, uniprot id "
    upTabFname = join(uniprotDir, "uniprot.tab")
    ret = []
    ofh = open(pairFname, "w")
    entrezToUp, upToSym, upToGb, upToRefseq = parseUniprot(uniprotDir, taxId)
    for upId, refseqIds in upToRefseq.iteritems():
        for refseq in refseqIds:
            ofh.write("%s\t%s\n" % (upId, refseq))
    ofh.close()
    return ofh.name
        
def upRefseqMap(taxId, uniprotDir):
    " map from uniprot to refseq in a psl "
    upFaName  = join(uniprotDir, "uniprot.%d.fa.gz" % taxId)
    tmpDir    = join(pubConf.mapReduceTmpDir, "uniprotRefseqMap")
    pairFname = join(tmpDir, "upToRefseq.pairs")
    mapFname  = join(mutDataDir, "upToRefseq.psl")

    makeOrCleanDir(tmpDir)
    writeUpRefseq(taxId, uniprotDir, pairFname)
    mapProtToRefseq(taxId, tmpDir, upFaName, pairFname, mapFname)
    

def mapProtToRefseq(taxId, tmpDir, protFaName, pairFname, mapFname):
    """ create a psl file with the best mapping protein -> refseq 
    
    input is protein fa and a file with pairs protein<->refseq
    output goes into mapFname
    """
    logging.debug("mapping %s to refseq, pairs in %s, tmpdir %s" % (protFaName, pairFname, tmpDir))
    refseqFname = join(mutDataDir, "refseq.%s.trans.fa" % str(taxId))
    BLASTDIR="/cluster/bin/blast/x86_64/blast-2.2.16/bin"
    targetFname = join(tmpDir, "refseq.fa")
    logging.info("Copying %s to %s" % (refseqFname, targetFname))
    shutil.copy(refseqFname, targetFname)

    # split uniprot into pieces 
    queryDir = join(tmpDir, "queries")

    makeOrCleanDir(queryDir)
    cmd = "gunzip %s -c | faSplit about stdin 2500 %s/" % (protFaName, queryDir)
    runCommand(cmd)
    # index refseq for blast
    cmd = "%s/formatdb -i %s -p F" % (BLASTDIR, targetFname)
    runCommand(cmd)

    # make dir for the output psl files
    pslDir = join(tmpDir, "psl")
    makeOrCleanDir(pslDir)

    # create joblist
    jbl = open(join(tmpDir, "jobList"), "w")
    logging.info("Created %s" % jbl.name)
    faFnames = glob.glob(join(queryDir, "*.fa"))
    logging.debug("Found %d part files" % len(faFnames))
    jobScriptFname = join(pubConf.ucscScriptDir, "mapUniprot_doBlast")
    for fname in faFnames:
        pslName = join(basename(fname))
        cmd = "%(jobScriptFname)s %(targetFname)s %(fname)s {check out exists %(pslDir)s}" % locals()
        jbl.write(cmd+"\n")
    jbl.close()

    # run
    cmd = 'ssh swarm "cd %s && para stop && para make jobList"' % tmpDir
    runCommand(cmd)

    # pick the best alignment for each protein 
    cmd = """ find %(pslDir)s -name 'psl.*' | xargs cat | pslSelect -qtPairs=%(pairFname)s stdin stdout | sort -k 10,10 | pslCDnaFilter stdin -minQSize=20 -ignoreNs -globalNearBest=0 -bestOverlap -filterWeirdOverlapped stdout | sort | uniq > %(mapFname)s""" % locals()
    runCommand(cmd)
    logging.info("Wrote results to %s" % mapFname)

def dbSnp(taxId, mutDataDir):
    assert(taxId==9606)
    tmpDir = pubConf.getTempDir()
    fastTmp = pubConf.getFastTempDir()

    snpFname = join(tmpDir, "snp137.tab")
    logging.info("getting snp137 into file %s" % snpFname)
    if not isfile(snpFname):
        cmd = '''hgsql hg19 -NB -e 'select chrom, chromStart, chromEnd, name from snp137' > %s''' % snpFname
        maxCommon.runCommand(cmd)
    else:
        logging.info("%s already exists, delete if you want to restart" % snpFname)

    # dbm has a huge problem when rec count > 25 million, so we split it
    dbSnpDbms = {}
    count = 0
    for line in open(snpFname):
        if count % 1000000 == 0:
            print "%d" % count
        count += 1
        chrom, start, end, name = line.rstrip("\n").split("\t")
        # ignore haps and _gl
        if "hap" in chrom or "_gl" in chrom:
            continue
        packCoord = struct.pack("<ll", int(start), int(end))
        # lazily create dbms, one for each chrom
        if chrom not in dbSnpDbms:
            fname = join(fastTmp, "snp137.%s.dbm" % chrom)
            logging.info("Creating %s" % fname)
            dbm = gdbm.open(fname, "nf")
            dbSnpDbms[chrom] = (fname, dbm)
        else:
            dbm = dbSnpDbms[chrom][1]
        packRsId = struct.pack("<l", int(name[2:]))
        dbm[packCoord] = packRsId

    for tmpFname, dbm in dbSnpDbms.values():
        dbm.close()
        finalDbmName = join(mutDataDir, basename(tmpFname))
        shutil.copy(tmpFname, finalDbmName)
        os.remove(tmpFname)
        logging.info("Copied %s to %s and deleted temp" % (tmpFname, finalDbmName))

def loadPslToDbm(pslFname, dbmFname, isProt=False):
    " load psl file into a compressed dbm file "
    qNamePsls = defaultdict(list)
    for line in open(pslFname):
        line = line.rstrip("\n")
        qName = line.split("\t")[9]
        if isProt:
            p = Psl(line.split("\t"))
            p.protToNa()
            line = str(p)
        qNamePsls[qName].append(line)

    # write to dbm as \n separated psl lines indexed by qName
    dbm = gdbm.open(dbmFname, "nf")
    for qName, pslLines in qNamePsls.iteritems():
        dbm[qName]= zlib.compress("\n".join(pslLines))
    dbm.close()
    logging.info("Wrote to %s" % dbmFname)

def refseqMap(taxId, mutDataDir):
    " get refseq -> genome map from browser "
    db = taxToDb[taxId]
    refPslFname = join(mutDataDir, "refGene.%d.psl" % taxId)
    #refCdsFname = join(mutDataDir, "refGene.%d.cds" % taxId)
    #cmd = 'genePredToFakePsl %s refGene %s %s' % (db, refPslFname, refCdsFname)
    #runCommand(cmd)
    cmd = 'hgsql -NB hg19 -e "select * from refSeqAli" | cut -f2- > %s' % refPslFname
    #shutil.copy("/cluster/data/genbank/data/aligned/refseq.59/hg19/full/mrna.native.psl.gz",
        #refPslFname)
    runCommand(cmd)

    refPslDbmFname = join(mutDataDir, "refGene.%d.psl.dbm" % taxId)
    loadPslToDbm(refPslFname, refPslDbmFname):

def main(args, options):
    pubGeneric.setupLogging(progFile, options)

    step = args[0]
    uniprotDir = join(pubConf.dbRefDir)
    global mutDataDir
    mutDataDir = join(pubConf.staticDataDir, "mutFinder")
    taxId = 9606

    allSteps = ["genePmids", "geneRefseq", "refseq", "seqs", "uniprot", "refseqMap", "uniprotMap", "snp"]
    if step not in allSteps:
        logging.error("Unknown step %s" % step)
        logging.info("Available steps are: %s" % ",".join(allSteps))
        sys.exit(1)

    if step=="genePmids" or step=="all":
        # parse entrez gene into dict pmid -> list of entrez ids
        entrezFname = join(mutDataDir, "pmid2entrez.dbm")
        dbm = gdbm.open(entrezFname, "nf")
        parseEntrezGenePmids(taxId, dbm)
        dbm.close()
        logging.info("Wrote pmid -> entrez to %s" % entrezFname)

    if step=="geneRefseq" or step=="all":
        outFname = join(mutDataDir, "entrezToRefseq.%s.tab" % taxId)
        parseEntrezGeneRefseq(taxId, outFname)

    if step=="refseq" or step=="all":
        parseRefseq(taxId, mutDataDir)

    if step=="seqs" or step=="all":
        loadSeqs(taxId, mutDataDir, uniprotDir)

    if step=="refseqMap" or step=="all":
        refseqMap(taxId, mutDataDir)

    if step=="uniprotMap" or step=="all":
        upRefseqMap(taxId, uniprotDir)

    if step="oldRefseq":
        downloadOldRefseq(taxId)

    if step=="snp" or step=="all":
        dbSnp(taxId, mutDataDir)

    if step=="uniprot" or step=="all":
        data = {}
        # parse uniprot seqs (get all variants)
        faFname = join(uniprotDir, "uniprot.%s.var.fa.gz" % str(taxId))
        seqDict = maxbio.parseFastaAsDict(faFname)
        data[taxId] = {}
        data[taxId]["upSeqs"] = seqDict
        logging.info("Found %s sequences" % len(seqDict))

        # parse entrez -> uniprot id and up -> symbol and up->genbank
        entrezToUp, upToSym, upToGb, upToRefseq = parseUniprot(uniprotDir, taxId)
        data[taxId]["entrezToUp"] = entrezToUp
        data[taxId]["upToSym"] = upToSym
        data[taxId]["upToGbs"] = upToGb

        # write to marshal file
        mutDataDir = join(pubConf.staticDataDir, "mutFinder")
        if not isdir(mutDataDir):
            os.makedirs(mutDataDir)
        mutDataFname = join(mutDataDir, "uniprot.tab.marshal")
        marshal.dump(data, open(mutDataFname, "w"))
        logging.info("Wrote to %s" % mutDataFname)

        # also write to tab file
        mutDataFname = join(mutDataDir, "uniprot.tab")
        ofh = open(mutDataFname,"w")
        ofh.write("geneId\tuniprotId\tuniprotSym\tuniprotGbAcc\n")
        noSym = 0
        for geneId, upId in entrezToUp.iteritems():
            sym = upToSym.get(upId, None)
            if sym==None:
                sym=""
                noSym +=1
            gbAccs = ",".join(upToGb[upId])
            row = [str(geneId), upId, sym, gbAccs]
            ofh.write("\t".join(row)+"\n")
        ofh.close()
        logging.info("No sym: %d" % noSym)
        logging.info("Wrote to %s" % mutDataFname)

    #if step=="ncbi" or step=="all":
        #entrezTabFname = join(pubConf.ncbiGenesDir, "gene2refseq.gz") 
        #logging.info("Parsing gene<->refseq ids out of %s" % entrezTabFname)
        #rnaIds = []
        #protIds = []
        #for line in gzip.open(entrezTabFname):
        #    if line.startswith("#"):
        #        continue
        #    fields = line.rstrip("\n").split("\t")
        #    if int(fields[0])!=taxId:
        #        continue
        #    rnaId = fields[3]
        #    protId = fields[5]
        #    rnaIds.append("*%s*"%rnaId)
        #    protIds.append("*%s*" % protId)

        #rnaIds = list(set(rnaIds))
        #protIds = list(set(protIds))
        #logging.info("RefSeq: Got %d rnaIds, %d protIds" % (len(rnaIds), len(protIds)))

        # get the genbank acc # of all uniprot sequences
        #entrezToUp, upToSym, upToGb = parseUniprot(uniprotDir, taxId)
        #gbIds = set()
        #for gbList in upToGb.values():
        #    gbIds.update(gbList)
        #gbIds = list(gbIds)

        #idFname = "/tmp/ncbiIds.txt"

        #ofh = open(idFname, "w")
        #ofh.write("\n".join(protIds))
        #ofh.write("\n")
        #ofh.write("\n".join(rnaIds))
        #ofh.write("\n")
        #ofh.write("\n".join(gbIds))
        #ofh.close()

if len(args)==0:
    parser.print_help()
    exit(1)

main(args, options)

