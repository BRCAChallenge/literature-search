#!/usr/bin/env python
# prepare the data for the mutation finder
# e.g. uniprot sequences, entrez mapping, etc

# load default python packages
import sys, logging, optparse, os, glob, shutil, gzip, collections, marshal, gdbm, re, zlib, cPickle
import struct
import urllib, urllib2
import xml.etree.ElementTree as et
from os.path import *
from collections import defaultdict

# add <scriptDir>/lib/ to package search path
progFile = os.path.abspath(sys.argv[0])
progDir  = os.path.dirname(progFile)
pubToolsLibDir = os.path.join(progDir, "lib")
sys.path.insert(0, pubToolsLibDir)

# now load our own libraries
import pubConf, pubGeneric, util, maxbio, maxCommon, pslMapBed
from os.path import *

# === COMMAND LINE INTERFACE, OPTIONS AND HELP ===
parser = optparse.OptionParser("""usage: %prog entrez|other|all: reformat uniprot sequences, entrez data etc for mutation finder and write to tools/data/mutFinder
""")

parser.add_option("-d", "--debug", dest="debug", action="store_true", help="show debug messages") 
parser.add_option("-v", "--verbose", dest="verbose", action="store_true", help="show more debug messages")
(options, args) = parser.parse_args()

# ==== FUNCTIONS =====
taxToDb = {9606 : "hg19"}

def writeGzDict(data, fname):
    logging.info("Writing %d keys to %s" % (len(data), fname))
    ofh = gzip.open(fname, "w")
    for key, valList in data.iteritems():
        for val in valList:
            assert("|" not in val)
        ofh.write("%s\t%s\n" % (key, "|".join(valList)))

def parseUniprot(uniprotDir, taxId):
    """ parse uniprot tab and return as three dicts 
        entrezId -> upId
        upId -> symbol
        upId -> list of gb accessions
    """
    tabFname = join(uniprotDir, "uniprot.tab")
    logging.info("Parsing entrez <-> uniprot from %s" % tabFname)
    entrezToUp = {}
    upToSym = {}
    upToGb = {}
    entrezCount = 0
    duplCount = 0
    for row in maxCommon.iterTsvRows(tabFname):
        if int(row.taxonId)==taxId:
            if row.geneName!="":
                upToSym[row.acc] = row.geneName
            if row.ncbiGene!="":
                ncbiGenes = row.ncbiGene.split("|")
                for ncbiGene in ncbiGenes:
                    ncbiGene = int(ncbiGene)
                    entrezCount += 1
                    if ncbiGene in entrezToUp:
                        duplCount +=1
                    entrezToUp[ncbiGene] = row.acc
            if row.embl!="":
                emblIds = row.embl.split("|")
                upToGb[row.acc] = emblIds
    logging.info("%d entrez -> uniprot links, skipped duplicates: %d" % (entrezCount, duplCount))
    return entrezToUp, upToSym, upToGb

def parseEntrezGeneRefseq(taxId, outFname):
    " create a tab-sep file with entrezGene, comma sep refseqIds, comma sep refseqProtIds"
    fname = join(pubConf.ncbiGenesDir, "gene2refseq.gz")
    logging.info("Parsing %s" % fname)
    # parse refseq into dicts
    refseqs = {}
    refprots = {}
    refsym = {}
    for line in gzip.open(fname):
       if not line.startswith("9606"):
           continue
       fs = line.strip("\n").split("\t")
       if not fs[0]=="9606":
           continue
       #print fs
       #if len(fs)<7:
            # some genes have no refseq info
            #continue
       tax, geneId, desc, refseqId, gir, refProtId, gip = fs[:7]
       sym = fs[15]
       if desc=="SUPPRESSED":
           continue
       if sym!="-":
           refsym[int(geneId)] = sym
       if refseqId!="-":
           refseqs.setdefault(int(geneId), set()).add(refseqId)
       if refProtId!="-":
           refprots.setdefault(int(geneId), set()).add(refProtId)

    # output dicts to tab sep file 
    logging.info("tab output...")
    ofh = open(outFname, "w")
    ofh.write("\t".join(["entrezId", "sym", "refseqIds", "refseqProtIds"]))
    ofh.write("\n")
    for geneId, refseqIds in refseqs.iteritems():
        refseqProtIds = refprots.get(geneId, [])
        sym = refsym.get(geneId, "")
        row = [str(geneId), sym, ",".join(refseqIds), ",".join(refseqProtIds)]
        ofh.write("\t".join(row))
        ofh.write("\n")
    ofh.close()
    logging.info("Wrote %s" % outFname)

    # write to marshal file
    outFname += ".marshal"
    data = {}
    data["entrez2refseqs"]  = refseqs
    data["entrez2refprots"] = refprots
    data["entrez2sym"] = refsym
    marshal.dump(data, open(outFname, "w"))
    logging.info("Wrote %s" % outFname)
    
def parseEntrezGenePmids(taxId, dbm):
    pmid2geneFname = join(pubConf.ncbiGenesDir, "gene2pubmed.gz")
    # at ucsc: /hive/data/outside/ncbi/genes/gene2pubmed.gz
    logging.info("Parsing %s" % pmid2geneFname)
    pmidToEntrez = {}
    for line in gzip.open(pmid2geneFname):
        if line.startswith("#"):
            continue
        row = line.rstrip("\n").split("\t")
        rowTax, entrezId, pmid = row
        if int(rowTax)==int(taxId):
            pmidToEntrez.setdefault(int(pmid), []).append(entrezId)
    logging.info("Taxon %d: found entrez ids for %s pmids" % (int(taxId), len(pmidToEntrez)))

    logging.info("Writing to dbm file")
    for pmid, entrezList in pmidToEntrez.iteritems():
        dbm[str(pmid)] = ",".join(entrezList)

def faToDbm(faName, dbm):
    logging.info("indexing %s as dbm" % faName)
    faSizeOfh = open(faName+".size", "w")
    logging.info("Reading %s" % faName)
    for seqId, seq in maxbio.parseFasta(faName):
        #if "prot" in faOfh.name:
            #print seqId, faOfh.name
        dbm[seqId] = zlib.compress(seq)
        #seqLen = len(seq)
        #if isProt:
            #seqLen = 3 * seqLen
        #faSizeOfh.write("%s\t%s\n" % (refseqId, seqLen))
    logging.info("Converted %s to dbm" % (faName))

def tabToDbm(fname):
    dbmFname = fname+".dbm"
    logging.info("Indexing %s to %s" % (fname, dbmFname))
    dbm = gdbm.open(dbmFname, "nf")
    for row in maxCommon.iterTsvRows(fname):
        key, val = row
        dbm[key] = val
    dbm.close()

def parseRa(raName, tabName):
    logging.info("Parsing ra")
    ofh = open(tabName, "w")
    ofh.write("refSeq\trefProt\tcdsStart\n")
    id = None
    cds = None
    prt = None
    data = {}
    skipRec = False
    skipCount = 0
    accList = []
    for line in open(raName):
        if line.startswith("acc"):
            id = line.rstrip("\n").split()[1]
            continue
        if line.startswith("ver"):
            ver = line.rstrip("\n").split()[1]
            continue
        if line.startswith("prt"):
            prt = line.rstrip("\n").split()[1]
            continue
        if line.startswith("cds"):
            cds = line.rstrip("\n").split()[1].split(".")[0]
            if "join" in line:
                skipRec = True
            continue
        if line=="\n" and id!=None and cds!=None and prt!=None:
            if skipRec:
                skipCount += 1
            else:
                acc = id+"."+ver
                row = [acc, prt, cds]
                ofh.write("\t".join(row))
                ofh.write("\n")
                accList.append(acc)
            id = None
            cds = None
            prt = None
            skipRec = False
    ofh.close()
    logging.info("Wrote cds and pep/refseq assignment to %s" % tabName)
    logging.info("Skipped %d records" % skipCount)
    return accList

def makeOldAccs(newAccList):
    " given a list of things like NM_000325.5, return NM_000325.4, NM_000325.3, etc. "
    oldAccs = []
    for fullAcc in newAccList:
        acc, ver = fullAcc.split(".")
        ver = int(ver)
        if ver != 1:
            for i in range(1, ver):
                oldAccs.append(acc+"."+str(i))
    return oldAccs

def chunkedDownloadFromEutils(accs, outFh, format="fasta"):
    " download in chunks of 5000 accs from eutils "
    chunkMax = 5000
    for chunkStart in range(0, len(accs), chunkMax):
        logging.info("Chunkstart is %d" % chunkStart)
        downloadFromEutils(accs[chunkStart:chunkStart+chunkMax], outFh, format="fasta")

def downloadFromEutils(accs, outFh, format="fasta"):
    " download accession via eutils "
    # query and put into history
    logging.info("Running eutils search for %d accessions" % len(accs))
    accFull = [acc for acc in accs]
    query = " OR ".join(accFull)
    base = "http://eutils.ncbi.nlm.nih.gov/entrez/eutils"
    url = base+'/esearch.fcgi';
    data = urllib.urlencode({"db":"nucleotide", "term":query, "usehistory":"y"})
    logging.debug("Data: %s" % data)
    req  = urllib2.Request(url, data)
    try:
        xmlStr = urllib2.urlopen(req).read()
    except urllib2.HTTPError:
        logging.error("Error when searching")
        raise
    logging.debug("XML reply: %s" % xmlStr)

    # now parse xml to get history IDs
    root = et.fromstring(xmlStr)
    try:
        webEnv = root.find("WebEnv").text
        key = root.find("QueryKey").text
    except AttributeError:
        logging.info("Error, XML is %s" % xmlStr)
        raise
    count = int(root.find("Count").text)
    logging.info("Got history id %s, found %d sequences" % (webEnv, count))

    # fetch
    retMax = 1000
    for retStart in range(0, count, retMax):
        logging.info("Retrieving up to %d records, start %d..." % (retMax, retStart))
        url = base + "/efetch.fcgi?db=nuccore&query_key=%(key)s&WebEnv=%(webEnv)s&" \
            "retstart=%(retStart)s&retmax=%(retMax)s&rettype=%(format)s&retmode=text" % locals();
        resp = urllib2.urlopen(url).read()
        outFh.write(resp)

def parseRefseq(taxId, mutDataDir):
    """ parse refseq sequences as compressed values to gdbm file """
    assert(taxId==9606)

    # get prot <-> trans assignment
    raName =  join(mutDataDir, "refseq.%s.ra"  % str(taxId))
    logging.info("Getting ra to %s" % raName)
    cmd = "gbGetSeqs -gbRoot=/hive/data/outside/genbank RefSeq mrna %s -get=ra -db=hg19 -inclVersion -native" % raName
    maxCommon.runCommand(cmd)
    refseqInfoFname =  join(mutDataDir, "refseqInfo.tab")
    accList = parseRa(raName, refseqInfoFname)
    os.remove(raName)

    # get old refseqs, too
    #oldRefseqFname = join(mutDataDir, "oldRefseq.%s.gb")
    #logging.info("Downloading old refseqs to %s" % oldRefseqFname)
    #oldAccs = makeOldAccs(accList)
    #outFh = open(oldRefseqFname, "w")
    #chunkedDownloadFromEutils(oldAccs, outFh)
    #outFh.write("\n".join(oldAccs[:1000]))
    #assert(False)

    # get fastas
    cmdTemp = "gbGetSeqs -gbRoot=/hive/data/outside/genbank RefSeq %s %s -db=hg19 -inclVersion"
    transFaName = join(mutDataDir, "refseq.%s.trans.fa" % str(taxId))
    protFaName =  join(mutDataDir, "refseq.%s.prot.fa"  % str(taxId))
    for seqType, fname in [("mrna", transFaName), ("pep", protFaName)]:
        logging.info("Getting data for %s" % seqType)
        cmd = cmdTemp % (seqType, fname)
        maxCommon.runCommand(cmd)
    logging.info("Wrote fastas to %s and %s" % (transFaName, protFaName))

    # index fastas
    dbmFname = join(mutDataDir, "refGeneSeqs.dbm")
    dbm = gdbm.open(dbmFname, "nf")
    faToDbm(transFaName, dbm)
    faToDbm(protFaName, dbm)
    dbm.close()

    logging.info("Wrote compressed refseq seqs to %s" % dbmFname)

def dbSnp(taxId, mutDataDir):
    assert(taxId==9606)
    tmpDir = pubConf.getTempDir()
    fastTmp = pubConf.getFastTempDir()

    snpFname = join(tmpDir, "snp137.tab")
    logging.info("getting snp137 into file %s" % snpFname)
    if not isfile(snpFname):
        cmd = '''hgsql hg19 -NB -e 'select chrom, chromStart, chromEnd, name from snp137' > %s''' % snpFname
        maxCommon.runCommand(cmd)
    else:
        logging.info("%s already exists, delete if you want to restart" % snpFname)

    snpDbmNameTemp = join(fastTmp, "snp137.dbm")
    dbm = gdbm.open(snpDbmNameTemp, "nf")
    count = 0
    for line in open(snpFname):
        if count % 1000000 == 0:
            print "%d" % count
        count += 1
        chrom, start, end, name = line.rstrip("\n").split("\t")
        packCoord = maxbio.packCoord(chrom, start, end)
        if packCoord==None:
            continue
        packRsId = struct.pack("l", int(name[2:]))
        dbm[packCoord] = packRsId
        count += 1
    dbm.close()
    logging.info("Wrote %s, copying to final dest %s" % (snpDbmNameTemp, snpDbmName))

    snpDbmName = join(mutDataDir, "snp137.dbm")
    shutil.copy(snpDbmNameTemp, snpDbmName)
    os.remove(snpDbmNameTemp)
    logging.info("Wrote %s, deleted %s" % (snpDbmName, snpDbmNameTemp))


def refseqMap(taxId, mutDataDir):
    " get refseq -> genome map from browser "
    db = taxToDb[taxId]
    refPslFname = join(mutDataDir, "refGene.%d.psl" % taxId)
    refCdsFname = join(mutDataDir, "refGene.%d.cds" % taxId)
    cmd = 'genePredToFakePsl %s refGene %s %s' % (db, refPslFname, refCdsFname)
    maxCommon.runCommand(cmd)

    # index by qname
    qNamePsls = defaultdict(list)
    for line in open(refPslFname):
        line = line.rstrip("\n")
        qName = line.split("\t")[9]
        #if isProt:
            #psl.protToNa()
        qNamePsls[qName].append(line)

    # write to dbm as \n separated psl lines indexed by qName
    refPslDbmFname = join(mutDataDir, "refGene.%d.psl.dbm" % taxId)
    refPslDbm = gdbm.open(refPslDbmFname, "nf")
    for qName, pslLines in qNamePsls.iteritems():
        refPslDbm[qName]= "\n".join(pslLines)
    refPslDbm["hi"] = "there"
    refPslDbm.close()
    logging.info("Wrote to %s" % refPslDbmFname)

    # pickle
    #queryToPsl = pslMapBed.indexPsls(refPslFname)
    #refPslFname = join(mutDataDir, "refGene.%d.psl.cpickle" % taxId)
    #cPickle.dump(queryToPsl, open(refPslFname, "w"))

def main(args, options):
    pubGeneric.setupLogging(progFile, options)

    step = args[0]
    uniprotDir = join(pubConf.dbRefDir)
    mutDataDir = join(pubConf.staticDataDir, "mutFinder")
    taxId = 9606

    allSteps = ["genePmids", "geneRefseq", "refseq", "uniprot", "refseqMap", "snp"]
    if step not in allSteps:
        logging.error("Unknown step %s" % step)
        logging.info("Available steps are: %s" % ",".join(allSteps))
        sys.exit(1)

    if step=="genePmids" or step=="all":
        # parse entrez gene into dict pmid -> list of entrez ids
        entrezFname = join(mutDataDir, "pmid2entrez.dbm")
        dbm = gdbm.open(entrezFname, "nf")
        parseEntrezGenePmids(taxId, dbm)
        dbm.close()
        logging.info("Wrote pmid -> entrez to %s" % entrezFname)

    if step=="geneRefseq" or step=="all":
        outFname = join(mutDataDir, "entrezToRefseq.%s.tab" % taxId)
        parseEntrezGeneRefseq(taxId, outFname)

    if step=="refseq" or step=="all":
        parseRefseq(taxId, mutDataDir)

    if step=="refseqMap" or step=="all":
        refseqMap(taxId, mutDataDir)

    if step=="snp" or step=="all":
        dbSnp(taxId, mutDataDir)

    if step=="uniprot" or step=="all":
        data = {}
        # parse uniprot seqs (get all variants)
        faFname = join(uniprotDir, "uniprot.%s.var.fa.gz" % str(taxId))
        seqDict = maxbio.parseFastaAsDict(faFname)
        data[taxId] = {}
        data[taxId]["upSeqs"] = seqDict
        logging.info("Found %s sequences" % len(seqDict))

        # parse entrez -> uniprot id and up -> symbol and up->genbank
        entrezToUp, upToSym, upToGb = parseUniprot(uniprotDir, taxId)
        data[taxId]["entrezToUp"] = entrezToUp
        data[taxId]["upToSym"] = upToSym
        data[taxId]["upToGbs"] = upToGb

        # write to marshal file
        mutDataDir = join(pubConf.staticDataDir, "mutFinder")
        if not isdir(mutDataDir):
            os.makedirs(mutDataDir)
        mutDataFname = join(mutDataDir, "uniprot.tab.marshal")
        marshal.dump(data, open(mutDataFname, "w"))
        logging.info("Wrote to %s" % mutDataFname)

        # also write to tab file
        mutDataFname = join(mutDataDir, "uniprot.tab")
        ofh = open(mutDataFname,"w")
        ofh.write("geneId\tuniprotId\tuniprotSym\tuniprotGbAcc\n")
        noSym = 0
        for geneId, upId in entrezToUp.iteritems():
            sym = upToSym.get(upId, None)
            if sym==None:
                sym=""
                noSym +=1
            gbAccs = ",".join(upToGb[upId])
            row = [str(geneId), upId, sym, gbAccs]
            ofh.write("\t".join(row)+"\n")
        ofh.close()
        logging.info("No sym: %d" % noSym)
        logging.info("Wrote to %s" % mutDataFname)

    #if step=="ncbi" or step=="all":
        #entrezTabFname = join(pubConf.ncbiGenesDir, "gene2refseq.gz") 
        #logging.info("Parsing gene<->refseq ids out of %s" % entrezTabFname)
        #rnaIds = []
        #protIds = []
        #for line in gzip.open(entrezTabFname):
        #    if line.startswith("#"):
        #        continue
        #    fields = line.rstrip("\n").split("\t")
        #    if int(fields[0])!=taxId:
        #        continue
        #    rnaId = fields[3]
        #    protId = fields[5]
        #    rnaIds.append("*%s*"%rnaId)
        #    protIds.append("*%s*" % protId)

        #rnaIds = list(set(rnaIds))
        #protIds = list(set(protIds))
        #logging.info("RefSeq: Got %d rnaIds, %d protIds" % (len(rnaIds), len(protIds)))

        # get the genbank acc # of all uniprot sequences
        #entrezToUp, upToSym, upToGb = parseUniprot(uniprotDir, taxId)
        #gbIds = set()
        #for gbList in upToGb.values():
        #    gbIds.update(gbList)
        #gbIds = list(gbIds)

        #idFname = "/tmp/ncbiIds.txt"

        #ofh = open(idFname, "w")
        #ofh.write("\n".join(protIds))
        #ofh.write("\n")
        #ofh.write("\n".join(rnaIds))
        #ofh.write("\n")
        #ofh.write("\n".join(gbIds))
        #ofh.close()

if len(args)==0:
    parser.print_help()
    exit(1)

main(args, options)

