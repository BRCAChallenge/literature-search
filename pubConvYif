#!/usr/bin/env python2.7

# first load the standard libraries from python
# we require at least python 2.5
#from sys import *
import sys
if sys.version_info[0]==2 and not sys.version_info[1]>=7:
    print "Sorry, this program requires at least python 2.7"
    exit(1)

# load default python packages
import logging, optparse, os, glob, zipfile, types, gzip, shutil
from os.path import *

# add <scriptDir>/lib/ to package search path
progFile = os.path.abspath(sys.argv[0])
progDir  = os.path.dirname(progFile)
pubToolsLibDir = os.path.join(progDir, "lib")
sys.path.insert(0, pubToolsLibDir)

# now load our own libraries
import pubGeneric, pubStore, pubConf, maxCommon

# === CONSTANTS ===================================
# === COMMAND LINE INTERFACE, OPTIONS AND HELP ===
parser = optparse.OptionParser("""usage: %prog [options] <inDir> <outDir> - convert Yale Image Finder text

articleIds are simply identifierStart["yif"]+the PMCID as given by YIF

example:
pubConvYif /hive/data/outside/pubs/medline/ /hive/data/inside/pubs/text/medline

""")

parser.add_option("-d", "--debug", dest="debug", action="store_true", help="show debug messages") 
parser.add_option("", "--minId", dest="minId", action="store", help="numerical IDs written to the pubStore start at this number times one billion to prevent overlaps of numerical IDs between publishers, default %default", default=pubConf.identifierStart["medline"]) 
#parser.add_option("-c", "--cluster", dest="cluster", action="store_true", help="use cluster for conversion", default=None) 
(options, args) = parser.parse_args()

# ==== FUNCTIONs =====
def submitJobs(inDir, outDir, minId, idStep, useCluster):
    " create one job per input medline file, process only new files "
    flagFnames = glob.glob(join(inDir, "newVersion.*"))
    assert(len(flagFnames))<=1
    if len(flagFnames)!=0:
        flagFname = flagFnames[0]
        oldVersionId = flagFname.split(".")[-1]
        oldDirBak = outDir+"."+oldVersionId
        logging.warn("New baseline came in. Removing all old data. Renaming %s to %s" % (outDir, oldDirBak))
        shutil.move(outDir, oldDirBak)
        logging.warn("Recreating clean %s" % outDir)
        os.makedirs(outDir)

        newFlag = join(outDir, basename(flagFname))
        logging.warn("Dropping flagfile %s" % newFlag)
        open(newFlag, "w").write("")

        logging.warn("Removing old flagfile %s" % flagFname)
        os.remove(flagFname)

    updateId, firstArticleId, doneFiles = pubStore.parseUpdatesTab(outDir, minArticleId=minId)

    medlineFnames = glob.glob(join(inDir, "*.xml.gz"))
    if len(medlineFnames)==0:
        medlineFnames = glob.glob(join(inDir, "*.xml"))
    if len(medlineFnames)==0:
        logging.error("No gz or xml files found in %s" % inDir)
        sys.exit(1)

    doneFiles = set(doneFiles)
    if useCluster:
        runner = pubGeneric.makeClusterRunner(__file__, maxJob=pubConf.convertMaxJob)
    else:
        runner = maxRun.Runner(maxJob=pubConf.convertMaxJob)

    chunkArticleId = firstArticleId
    chunkId = 0
    newFiles = set()
    for medlineFname in medlineFnames:
        if basename(medlineFname) in doneFiles:
            continue
        outFname = os.path.join(outDir, "%d_%05d.articles.gz" % (updateId, chunkId))
        maxCommon.mustNotExist(outFname)
        command = "%s %s {check in exists %s} {check out exists+ %s} %d" % (sys.executable, progFile, medlineFname, outFname, chunkArticleId)
        runner.submit(command)
        chunkArticleId += idStep
        chunkId += 1
        newFiles.add(basename(medlineFname))
    runner.finish(wait=True)
    pubStore.appendToUpdatesTxt(outDir, updateId, chunkArticleId, newFiles)

def convertOneChunk(fileMinId, inFile, outFile):
    """ 
    convert one medlinefile to one pubtools file
    """ 
    store = pubStore.PubWriterFile(outFile)

    logging.debug("Reading %s" % inFile)
    if inFile.endswith(".gz"):
        xmlString = gzip.open(inFile).read()
    else:
        xmlString = open(inFile).read()

    logging.debug("Writing to %s" % outFile)
    articleId = int(fileMinId)
    # parse & write to output
    for articleData in pubPubmed.parsePubmedMedlineIter(xmlString, fromMedline=True):
        logging.debug("Writing article %s" % str(articleId))
        articleData["source"]="medline"
        articleData["origFile"]=inFile
        articleData["publisher"]="ncbi"
        store.writeArticle(articleId, articleData)
        articleId += 1
    store.close()

# ----------- MAIN --------------
# only for debugging
if options.parse!=None:
    fname = options.parse
    xmlString = open(fname).read()
    for articleData in pubPubmed.parsePubmedMedlineIter(xmlString, fromMedline=True):
        for key, val in articleData.iteritems():
            print key, val.encode("utf8")
    sys.exit(0)
    
if args==[]: 
    parser.print_help()
    exit(1)

# normal operation
inDir, outDir = args[:2]
maxCommon.mustExist(inDir)

minId = options.minId
maxIdPerFile = options.idsPerFile

pubGeneric.setupLogging(progFile, options)

if os.path.isdir(inDir):
    maxCommon.mustExistDir(outDir)
    #maxCommon.mustBeEmptyDir(outDir)
    submitJobs(inDir, outDir, minId, maxIdPerFile, options.cluster)
else:
    inFile = inDir
    outFile = outDir
    chunkMinId = args[2]
    convertOneChunk(int(chunkMinId), inFile, outFile)

if options.updateDb:
    tsvFnames = glob.glob(join(outDir,"*.articles.gz"))
    dbPath = join(outDir, "new.articles.db")
    pubStore.loadNewTsvFilesSqlite(dbPath, "articles", tsvFnames)
