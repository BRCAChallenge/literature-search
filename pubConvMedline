#!/usr/bin/env python2.7

# first load the standard libraries from python
# we require at least python 2.5
#from sys import *
import sys
if sys.version_info[0]==2 and not sys.version_info[1]>=7:
    print "Sorry, this program requires at least python 2.7"
    print "You can download a more current python version from python.org and compile it"
    print "into your homedir with 'configure --prefix ~/python'; make;"
    print "then run this program by specifying your own python executable like this: "
    print "   ~/python/bin/python ~/pubtools/pubtools"
    print "or add python/bin to your PATH before /usr/bin, then run pubtools itself"
    exit(1)

# load default python packages
import logging, optparse, os, glob, zipfile, types, gzip, shutil
from os.path import *

# add <scriptDir>/lib/ to package search path
progFile = os.path.abspath(sys.argv[0])
progDir  = os.path.dirname(progFile)
pubToolsLibDir = os.path.join(progDir, "lib")
sys.path.insert(0, pubToolsLibDir)

# now load our own libraries
import pubGeneric, maxRun, pubStore, pubConf, maxCommon, pubXml, pubPubmed

# === CONSTANTS ===================================
# === COMMAND LINE INTERFACE, OPTIONS AND HELP ===
parser = optparse.OptionParser("""usage: %prog [options] <inDir> <outDir> - convert medline .gz files to pubTools format and optionally write to a sqlite db

example:
pubConvMedline /hive/data/outside/pubs/medline/ /hive/data/inside/pubs/text/medline

Each file is a separate job, articleIds are assigned to each file such that each chunk starts at +300k articleIds
after the last chunk, to avoid overlaps

Download medline files from Medline ftp with pubGetMedline.

Or alternatively with something like:
    lftp -e 'set net:socket-buffer 4000000; connect ftp://ftp.nlm.nih.gov/nlmdata/.medleasebaseline/gz; mirror -c --parallel=8 .; quit'
""")

parser.add_option("", "--minId", dest="minId", action="store", help="numerical IDs written to the pubStore start at this number times one billion to prevent overlaps of numerical IDs between publishers, default %default", default=pubConf.identifierStart["medline"]) 
parser.add_option("", "--idsPerFile", dest="idsPerFile", action="store", help="number of identifiers per medline file. Reserver space for x entries in numerical namespace, default %default", default=300000) 
parser.add_option("", "--parse", dest="parse", action="store", help="for debugging, just parse one single xml file", default=None) 
parser.add_option("-u", "--updateDb", dest="updateDb", action="store_true", help="export new data to sqlite db defined in pubConf") 
pubGeneric.addGeneralOptions(parser)
(options, args) = parser.parse_args()

# ==== FUNCTIONs =====
def submitJobs(inDir, outDir, minId, idStep, useCluster):
    " create one job per input medline file, process only new files "
    flagFnames = glob.glob(join(inDir, "newVersion.*"))
    assert(len(flagFnames))<=1
    if len(flagFnames)!=0:
        flagFname = flagFnames[0]
        oldVersionId = flagFname.split(".")[-1]
        oldDirBak = outDir+"."+oldVersionId
        logging.warn("New baseline came in. Removing all old data. Renaming %s to %s" % (outDir, oldDirBak))
        shutil.move(outDir, oldDirBak)
        logging.warn("Recreating clean %s" % outDir)
        os.makedirs(outDir)

        newFlag = join(outDir, basename(flagFname))
        logging.warn("Dropping flagfile %s" % newFlag)
        open(newFlag, "w").write("")

        logging.warn("Removing old flagfile %s" % flagFname)
        os.remove(flagFname)

    updateId, firstArticleId, doneFiles = pubStore.parseUpdatesTab(outDir, minArticleId=minId)

    medlineFnames = glob.glob(join(inDir, "*.xml.gz"))
    if len(medlineFnames)==0:
        medlineFnames = glob.glob(join(inDir, "*.xml"))
    if len(medlineFnames)==0:
        logging.error("No gz or xml files found in %s" % inDir)
        sys.exit(1)

    doneFiles = set(doneFiles)
    if useCluster:
        runner = pubGeneric.makeClusterRunner(__file__, maxJob=pubConf.convertMaxJob)
    else:
        runner = maxRun.Runner(maxJob=pubConf.convertMaxJob)

    chunkArticleId = firstArticleId
    chunkId = 0
    newFiles = set()
    fCount = 0
    for medlineFname in medlineFnames:
        if basename(medlineFname) in doneFiles:
            continue
        fCount += 1
        outFname = os.path.join(outDir, "%d_%05d.articles.gz" % (updateId, chunkId))
        maxCommon.mustNotExist(outFname)
        command = "%s %s {check in exists %s} {check out exists+ %s} %d" % \
            (sys.executable, progFile, medlineFname, outFname, chunkArticleId)
        runner.submit(command)
        chunkArticleId += idStep
        chunkId += 1
        newFiles.add(basename(medlineFname))

    if fCount==0:
        logging.info("All files were already converted, nothing done")
        return 

    runner.finish(wait=True)
    pubStore.appendToUpdatesTxt(outDir, updateId, chunkArticleId, newFiles)

def convertOneChunk(fileMinId, inFile, outFile):
    """ 
    convert one medlinefile to one pubtools file
    """ 
    store = pubStore.PubWriterFile(outFile)

    logging.debug("Reading %s" % inFile)
    if inFile.endswith(".gz"):
        xmlString = gzip.open(inFile).read()
    else:
        xmlString = open(inFile).read()

    logging.debug("Writing to %s" % outFile)
    articleId = int(fileMinId)
    # parse & write to output
    for articleData in pubPubmed.parsePubmedMedlineIter(xmlString, fromMedline=True):
        logging.debug("Writing article %s" % str(articleId))
        articleData["source"]="medline"
        articleData["origFile"]=inFile
        articleData["publisher"]="ncbi"
        store.writeArticle(articleId, articleData)
        articleId += 1
    store.close()

# ----------- MAIN --------------
# only for debugging
if options.parse!=None:
    fname = options.parse
    xmlString = open(fname).read()
    for articleData in pubPubmed.parsePubmedMedlineIter(xmlString, fromMedline=True):
        for key, val in articleData.iteritems():
            print key, val.encode("utf8")
    sys.exit(0)
    
if args==[]: 
    parser.print_help()
    exit(1)

# normal operation
inDir, outDir = args[:2]
maxCommon.mustExist(inDir)

minId = options.minId
maxIdPerFile = options.idsPerFile

pubGeneric.setupLogging(progFile, options)

if os.path.isdir(inDir):
    maxCommon.mustExistDir(outDir)
    #maxCommon.mustBeEmptyDir(outDir)
    submitJobs(inDir, outDir, minId, maxIdPerFile, options.cluster)
else:
    inFile = inDir
    outFile = outDir
    chunkMinId = args[2]
    convertOneChunk(int(chunkMinId), inFile, outFile)

if options.updateDb:
    tsvFnames = glob.glob(join(outDir,"*.articles.gz"))
    dbPath = join(outDir, "articles.db")
    pubStore.loadNewTsvFilesSqlite(dbPath, "articles", tsvFnames)
