#!/usr/bin/env python

# first load the standard libraries from python
# we require at least python 2.7
#from sys import *
import sys
if sys.version_info[0]==2 and not sys.version_info[1]>=7:
    print "Sorry, this program requires at least python 2.7"
    print "You can download a more current python version from python.org and compile it"
    print "into your homedir (or anywhere) with 'configure --prefix ~/python27'; make;"
    print "then run this program by specifying your own python executable like this: "
    print "   ~/python27/bin/python <scriptFile>"
    print "or add ~/python27/bin to your PATH before /usr/bin"
    exit(1)

# load default python packages
import logging, optparse, os, collections, tarfile, mimetypes, tempfile, \
    copy, shutil, glob, time
from os.path import *

# add <scriptDir>/lib/ to package search path
progFile = os.path.abspath(sys.argv[0])
progDir  = os.path.dirname(progFile)
pubToolsLibDir = os.path.join(progDir, "lib")
sys.path.insert(0, pubToolsLibDir)

# now load our own libraries
import maxRun, pubStore, pubConf, pubGeneric, pubAlg, maxCommon
from maxCommon import *

# === COMMAND LINE INTERFACE, OPTIONS AND HELP ===
parser = optparse.OptionParser("""usage: %prog [options] <algorithmName> <in> <out> <opt1>=<val1> <opt2>=<val2> ...- run an algorithm on a directory of fulltext files and write results to out directory

<algorithmName> can be a script from the scripts directory, like "protSearch.py". (Extension optional)
By default, only a variable "headers" and a function "annotateFile" is needed in the script. If
the script contains classes to separate different functions, you need to add the class name, separated by
a ":", like "dnaSearch.py:Annotate"

<in> can be a directory or dataset name:
- search <in> for fulltext chunks (*.articles.gz and *.files.gz)
- for each chunk submit a cluster job to write results to <out>

""")

parser.add_option("-t", "--test", dest="test", action="store_true", help="run locally in single process, not on cluster or multi processes, to see error messages, dump annotations to stdout") 
#parser.add_option("", "--realTest", dest="realTest", action="store_true", help="spawn processes on local machine, like a cluster run, but error messages are visible, write annotations to final output files")
parser.add_option("-o", "--oneText", dest="oneText", action="store_true", help="like --test and <in> is just a single text file")
parser.add_option("-a", "--addFields", dest="addFields", action="append", help="add article fields to output files, e.g. pmid or doi. can be used several times")
parser.add_option("-l", "--limit", dest="limitJobs", action="store", type="int", help="limit jobs to X concurrent jobs on the cluster")
parser.add_option("", "--cat", dest="concat", action="store_true", help="write output to <out>.dir and concat all files to <out> when jobs are finished")
parser.add_option("-k", "--keepOldFiles", dest="keepOldFiles", action="store_true", help="do not wipe the output dir before running the jobs")
parser.add_option("-p", "--addPmid", dest="addPmid", action="store_true", help="try to add PMID to all output rows (this is the same as -a pmid)")
pubGeneric.addGeneralOptions(parser)
(options, args) = parser.parse_args()

# ==== FUNCTIONs =====
def checkCleanDir(outDir, keepOldFiles):
    if not isdir(outDir):
        logging.info("Creating dir %s" % outDir)
        os.makedirs(outDir)
    oldGzFiles = os.listdir(outDir)
    if len(oldGzFiles)>0 and not keepOldFiles:
        logging.info("Directory %s contains %d .gz files" % (outDir, len(oldGzFiles)))
        logging.info("Waiting for 3 secs, then deleting them")
        time.sleep(3)
        pm = maxCommon.ProgressMeter(len(oldGzFiles))
        for oldGzFname in oldGzFiles:
            os.remove(join(outDir, oldGzFname))
            pm.taskCompleted()

def testMode():
    reload(sys)
    sys.setdefaultencoding('utf-8')
    outName = "stdout"
    if options.oneText:
        inFiles = [inName]
    else:
        inDir = pubConf.resolveTextDir(inName.split(",")[0])
        inFiles = glob.glob(join(inDir, "*.articles.gz"))

    for inFname in inFiles:
        logging.info("Running on %s" % inFname)
        if options.oneText:
            logging.info("Running only on file %s" % options.oneText)
            reader = pubStore.PubReaderTest(inFname)
        else:
            reader = pubStore.PubReaderFile(inFname)
        pubAlg.runAnnotate(reader, alg, paramDict, outName)
    sys.exit(0)
# ----------- MAIN --------------
if args==[]: 
    parser.print_help()
    exit(1)

pubGeneric.setupLogging(progFile, options)

algName, inName, outName = args[:3]

assert("=" not in outName)
assert("=" not in inName)

paramStrings = args[3:]

paramDict = {}
paramDict = pubGeneric.stringListToDict(paramStrings)

# we can't test jython algorithms
if not algName.startswith("java"):
    alg = pubAlg.getAlg(algName, "Annotate") # makes sure that algName exists

paramDict["startAnnotId"] = 0
paramDict["addFields"] = options.addFields

if options.addPmid:
    if options.addFields==None:
        options.addFields=[]
    options.addFields.append("pmid")

if options.test or options.oneText:
    testMode()

inNames = inName.split(",")
checkCleanDir(outName, options.keepOldFiles)
maxJobs = None
if options.limitJobs:
    maxJobs = options.limitJobs
inBaseNames = [basename(x) for x in inNames]
batchBase = algName+"_"+"".join(inBaseNames)
runner = pubGeneric.makeClusterRunner(batchBase, maxJob=maxJobs)
pubAlg.annotate(algName, inNames, paramDict, outName, runner=runner, addFields=options.addFields, concat=options.concat)
