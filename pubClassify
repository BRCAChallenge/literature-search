#!/usr/bin/env python2.7

# script to run the various steps of the text classification pipeline

# first load the standard libraries from python
# we require at least python 2.5
#from sys import *
import sys
if sys.version_info[0]==2 and not sys.version_info[1]>=7:
    print "Sorry, this program requires at least python 2.7"
    sys.exit(1)

# load default python packages
import logging, optparse, os, glob, zipfile, types, gzip, shutil, subprocess, itertools, operator, gc
import marshal
from os.path import *
from collections import defaultdict

# add <scriptDir>/lib/ to package search path
progFile = os.path.abspath(sys.argv[0])
progDir  = os.path.dirname(progFile)
pubToolsLibDir = os.path.join(progDir, "lib")
sys.path.insert(0, pubToolsLibDir)

# now load our own libraries
import pubGeneric, maxRun, pubConf, maxCommon, pubExpMatrix, html, pubStore

# === CONSTANTS ===================================
# === COMMAND LINE INTERFACE, OPTIONS AND HELP ===
parser = optparse.OptionParser("""usage: %prog [options] <datasetList> <step> - script to run the various steps of the article classification pipeline

steps:
bestWords = create the list of best words for each database
wordCount = create document-frequency list of words, needs to be filtered with 
            google/BNC to be useful
            writes to data/wordList/wordList.raw.txt
tmatrix   = create training matrix from document collection (map/reduce)
            reads pmid lists from data/classify/pmids
            writes to <pubBase>/classify/tmatrix.tab
            rewrite training matrix to one file per biological database
            and train svmlight models
            writes to data/classify/models
dmatrix   = create big document matrix from document collection, by default
            runs only on 
            reads from article datasets
            writes to <pubBase>/classify/docMatrix.svml 
classify  = run models onto docMatrix and write to <pubBase>/classify/docClasses.tab
html      = generate html pages for databases and write to html directory

example:
pubFilter pmc,elsevier,crawler dmatrix

""")

#parser.add_option("", "--minId", dest="minId", action="store", help="numerical IDs written to the pubStore start at this number times one billion to prevent overlaps of numerical IDs between publishers, default %default", default=pubConf.identifierStart["medline"]) 
#parser.add_option("", "--parse", dest="parse", action="store", help="for debugging, just parse one single xml file", default=None) 
parser.add_option("", "--skipMap", dest="skipMap", action="store_true", help="skip all map steps")
parser.add_option("", "--onlyDbs", dest="onlyDbs", action="store", help="run only on a given db, not on all")
parser.add_option("-t", "--test", dest="test", action="store_true", help="only run the test, nothing else")
parser = pubGeneric.addGeneralOptions(parser)
(options, args) = parser.parse_args()
pubGeneric.setupLogging(__file__, options)

# ==== FILE NAMES ====

# set up two main dirs: one for static data from UCSC and one for local data
dataDir = pubConf.getStaticDataDir()
statClassDir = join(dataDir, "classify") # static data, part of code repo
varDir  = pubConf.pubsDataDir
varClassDir = join(varDir, "classify") # pipeline data, like temporary files and output

wordCountDir     = join(varClassDir, "wordListOutput")

rawWordListFname = join(statClassDir, "wordList.raw.txt")
wordListFname    = join(statClassDir, "wordList.txt.gz")

wordCountFname    = join(varClassDir, "wordCounts.marshal")
bestWordsDir     = join(statClassDir, "bestWords")

pmidListDir      = join(statClassDir, "trainPmids")

mapRedDir        = join(varClassDir, "mapReduceTemp")
tMatrixFname     = join(varClassDir, "allTrain.pmidsvml")
svmlDir         = join(statClassDir, "svmlInput")
modelDir         = join(statClassDir, "svmlModels")
alphaDir         = join(statClassDir, "svmlAlphas")
svmlBinDir      = pubConf.svmlBinDir

dMatrixFname     = join(varClassDir, "docs.svml")
dIdFname         = join(varClassDir, "docs.docIds")
categoryFname    = join(varClassDir, "docClasses.tab.gz")

classDir         = join(varClassDir, "svmlClasses")

if not isdir(varClassDir):
    os.makedirs(varClassDir)

# ==== FUNCTIONs =====
def parsePmids(inDir):
    """ parse pmids from dir and return as a set of all PMIDs and as a dict
        db -> list of (class, pmid) tuples, class is either "+1" or "-1"
    """
    logging.info("Parsing PMIDs")
    allPmids = set()
    pmidDbs = {}
    dbs = set()
    inFnames = glob.glob(join(inDir, "*.txt"))
    for inFname in inFnames:
        base = basename(inFname)
        if not (base.startswith("pos") or base.startswith("neg")):
            continue
        pmidClass, db = base.split(".")[:2]
        dbs.add(db)
        if pmidClass=="pos":
            svmlClass = "+1"
        elif pmidClass=="neg":
            svmlClass = "-1"
        else:
            assert(False)

        logging.debug("Parsing %s" % inFname)
        for line in open(inFname):
            pmid = line.strip()
            pmid = int(pmid)
            allPmids.add(pmid)
            pmidDbs.setdefault(pmid, []).append( (svmlClass, db) )
    logging.info("Read %d PMIDs from %s" % (len(allPmids), inDir))
    return allPmids, dbs, pmidDbs

def mkEmptyDir(dir):
    " make sure dir exists and is empty "
    if isdir(dir):
        shutil.rmtree(dir)
    if not isdir(dir):
        os.makedirs(dir)

def splitSvml(tMatrixFname, dbs, pmidDbs, svmlDir):
    """ create one svml output file in svmlDir per db in dbPmids and distribute the svml lines from
        tMatrixFname to the right files in svmlDir
    """
    dbOfh = {}
    fnames = []
    mkEmptyDir(svmlDir)

    for db in dbs:
        ofname = join(svmlDir, db+".svml")
        dbOfh[db] = open(ofname, "w")
        fnames.append(ofname)

    logging.debug("Rewriting %s" % tMatrixFname)
    for line in open(tMatrixFname):
        docId, featVec = line.split(" ", 1)
        pmid = int(docId.split("/")[2])
        for svmlClass, db in pmidDbs[pmid]:
            dbOfh[db].write(svmlClass+" ")
            dbOfh[db].write(featVec)

    logging.info("Wrote SVML files: %s" % " ".join(fnames))

def svmlLearn(svmlBinDir, svmlDir, modelDir, alphaDir, dbList):
    " run svml_learn on all .svml files in svmlDir "
    binPath = join(svmlBinDir, "svm_learn")
    if not isfile(binPath):
        raise Exception("%s does not exist" % binPath)
    mkEmptyDir(modelDir)
    mkEmptyDir(alphaDir)

    logging.info("Using SVML files in dir %s" % svmlDir)
    for svmlFname in glob.glob(join(svmlDir, "*.svml")):
        db = splitext(basename(svmlFname))[0]
        if dbList!=None and db not in dbList:
            continue
        modelFname = join(modelDir, db+".model")
        alphaFname = join(alphaDir, db+".alpha")
        logging.info("Running SVMlight for db %s" % db)
        cmd = [binPath, svmlFname, modelFname, "-a", alphaFname]
        subprocess.check_call(cmd)
    logging.info("alphaput written to %s and %s" % (modelDir, alphaDir))

def svmlClassify(svmlBinDir, svmlFname, modelDir, classDir, dbList):
    " run svml on all models from modelDir "
    mkEmptyDir(classDir)
    binPath = join(svmlBinDir, "svm_classify")
    if not isfile(binPath):
        raise Exception("%s does not exist" % binPath)

    logging.info("Classifying with SVMLight, feature file %s" % svmlFname)
    runner = pubGeneric.makeClusterRunner(__file__, algName="svmlClassify")
    for modelFname in glob.glob(join(modelDir, "*.model")):
        db = splitext(basename(modelFname))[0]
        if dbList!=None and db not in dbList:
            continue
        outFname = join(classDir, db+".classes")
        logging.info("Running on %s" % modelFname)
        cmd = [binPath, svmlFname, modelFname, "{check out line+ %s}" % outFname]
        cmd = " ".join(cmd)
        #logging.debug("command is %s" % cmd)
        runner.submit(cmd)
    runner.finish(wait=True)

def convertSvmlResults(dIdFname, classDir, categoryFname, dbList):
    """ combine svml output and article ids and write categories in 
    a format that is easier to parse: docId<tab>dbs (comma-sep) 
    """
    logging.info("Reading document identifiers from %s" % dIdFname)
    docIds = [l.strip() for l in open(dIdFname).readlines()]
    dbCounts = defaultdict(int)

    # create a dict with documentId/externalId -> list of databases
    docClasses = defaultdict(list)
    for classFname in glob.glob(join(classDir, "*.classes")):
        logging.info("Reading class assignment from %s" % classFname)
        classValues = [float(l.strip()) for l in open(classFname).readlines()]
        db = splitext(basename(classFname))[0]
        if dbList!=None and db not in dbList:
            continue
        assert(len(classValues)==len(docIds))
        for docId, classValue in itertools.izip(docIds, classValues):
            if classValue>0.0:
                docClasses[docId].append((db, classValue))
                dbCounts[db]+=1
    logging.info("processed %d articles" % len(docClasses))

    # write result to out file
    ofh = gzip.open(categoryFname, "w")
    ofh.write("articleId\texternalId\tclasses\tscores\n")
    for docId, catScores in docClasses.iteritems():
        artId, extId = docId.split("/")
        classes, scores = zip(*catScores) # weird python magic
        scores = [str(s) for s in scores]
        ofh.write("%s\t%s\t%s\t%s\n" % (artId, extId, ",".join(classes), ",".join(scores)))
    ofh.close()
    logging.info("Wrote class info to %s" % categoryFname)
    for db, dbCount in dbCounts.iteritems():
        logging.info("%s: %d assigned documents" % (db, dbCount))

def parseDocClasses(fname):
    " return dict db -> list of (articleId (INT!), score) "
    logging.info("Parsing %s" % fname)
    res = defaultdict(list)
    for row in maxCommon.iterTsvRows(fname):
        artId = int(row.articleId)
        classes = row.classes.split(',')
        scores = row.scores.split(',')
        for db, score in zip(classes, scores):
            res[db].append((artId, float(score)))
    return res

def makeHtml(categoryFname, outDir, dbList):
    " "
    dbArtIds = parseDocClasses(categoryFname)
    logging.info("Writing html")
    for db, artIdScores in dbArtIds.iteritems():
        if dbList!=None and db not in dbList:
            continue
        outfname = join(outDir, db+".html")
        logging.info("DB %s, filename %s" % (db, outfname))
        h = html.htmlWriter(outfname)
        h.head("Documents classified as: %s" % db)
        h.h4("Documents classified as: %s" % db)
        h.p()
        pm = maxCommon.ProgressMeter(len(artIdScores), stepCount=100)
        artIdScores.sort(key=operator.itemgetter(1), reverse=True) # to improve db speed
        for artId, score in artIdScores:
            art = pubStore.lookupArticleByArtId(artId)
            h.link(art["fulltextUrl"], art["title"])
            h.br()
            ref = ["score: "+str(score), art["journal"], art["year"], art["authors"]]
            h.writeLn(", ".join(ref))
            h.p()
            pm.taskCompleted()
        h.endHtml()
    
def readWordPmids(inDir):
    " parse wordCounts, return as dict word -> pmids "
    gc.disable()
    res = {}
    fnames = glob.glob(join(inDir, "*.tab.gz"))
    logging.info("Found %d files in %s" % (len(fnames), inDir))
    pm = maxCommon.ProgressMeter(len(fnames))
    pmids = set()
    duplPmidCount = 0
    for fname in fnames:
        logging.debug("Parsing %s")
        for row in maxCommon.iterTsvRows(fname):
            pmid = int(row.pmid)
            if pmid in pmids:
                logging.debug("duplicated pmid %d" % pmid)
                duplPmidCount += 1
                continue
            pmids.add(pmid)
            wordStr = row.wordCounts
            words = [w.split("=")[0] for w in wordStr.split(",")]
            for word in words:
                res.setdefault(word, []).append(pmid)
        pm.taskCompleted()
    gc.enable()
    logging.info("Featurecount: %d" % len(res))
    logging.info("PMID count: %d" % len(pmids))
    logging.info("Duplicated PMIDs: %d" % duplPmidCount)
    return res
    
def makeBestWords(dbList, wordCountFname, pmidDir, bestWordsDir):
    """ read word counts from dataset completely into memory, for each db, use 
      pmidList to separate into pos/neg and do a chi-square test to find best 
      words 
    """
    wordPmids = marshal.load(open(wordCountFname))
    for db in dbList:
        logging.info("Finding best words for db %s" % db)
        # parse pmids
        posPmids = set([int(l.strip()) for l in open(join(pmidDir, "pos."+db+".txt"))])
        negPmids = set([int(l.strip()) for l in open(join(pmidDir, "neg."+db+".txt"))])
        logging.debug("%d positive documents, %d background documents" % (len(posPmids), len(negPmids)))

        gc.disable()
        wordScores = []
        for word, pmidList in wordPmids.iteritems():
            wordPmids = set(pmidList)
            obsOvl = len(wordPmids.intersection(posPmids))
            if word=="drosophila":
                print wordPmids
            if obsOvl < 10:
                continue
            negOvl = len(wordPmids.intersection(negPmids))
            expOvl = int(float(negOvl) * len(posPmids))
            logging.debug("word %s: overlap with positives %d, overlap with background %d, expected overlap with positives %d" % (word, obsOvl, negOvl, expOvl))
            if expOvl==0:
                expOvl=1
            chiSq = (obsOvl-expOvl)**2 / float(expOvl)
            wordScores.append( (word, chiSq) )
        wordScores.sort(key=operator.itemgetter(1), reverse=True)
        gc.enable()

    print wordScores[:100]

def removeWords(wordCounts, minCount):
    logging.info("Removing words that occur less than %d times" % minCount)
    gc.disable()
    newDict = {}
    for key, valList in wordCounts.iteritems():
        if len(valList)>minCount:
            newDict[key] = valList
    gc.enable()
    return newDict

def writeWordPmids(wordPmids, wordCountFname):
    logging.info("Writing words")
    marshal.dump(wordPmids, open(wordCountFname, "w"))
    logging.info("Filesize: %3d MB" % (os.path.getsize(wordCountFname)/1000000))
    logging.info("Output written to %s" % wordCountFname)

def getDbList(pmidDir):
    " create a list of all possible DBs in pmidDir, those with a pos. and a neg.<db>.txt file "
    fnames = os.listdir(pmidDir)
    posNames = [fn.split(".")[1] for fn in fnames if fn.startswith("pos.") and fn.endswith(".txt")]
    negNames = [fn.split(".")[1] for fn in fnames if fn.startswith("neg.") and fn.endswith(".txt")]
    dbNames = set(posNames).intersection(negNames)
    return dbNames

def main(args, options):
    datasets, stepsString = args
    steps = stepsString.split(',')
    textDirs = pubConf.resolveTextDirs(datasets)
    dbList=getDbList(pmidListDir)

    if options.onlyDbs:
        dbList = options.onlyDbs.split(",")
        assert(len(set(dbList).intersection(dbList))==len(dbList)) # dbs have to exist in pmidListDir
    
    if "countWords" in steps:
        pass
        # submitWordCountJobs(..., wordCountDir)

    if "parseWords" in steps:
        wordPmids = readWordPmids(wordCountDir)
        wordPmids = removeWords(wordPmids, 100)
        writeWordPmids(wordPmids, wordCountFname)

    if "rankWords" in steps:
        print dbList
        makeBestWords(dbList, wordCountFname, pmidListDir, bestWordsDir)

    if "wordCount" in steps:
        runner = pubGeneric.makeClusterRunner(__file__, "pubClassify-wordCount")
        pubExpMatrix.buildWordList(runner, datasets, options.skipMap, wordListFname)
        logging.info("raw word list created, use your own command now to reduce it to something smaller")
        logging.info("e.g. cat %s | gawk '($2<50000) && ($2>100)' | cut -f1 | lstOp remove stdin /hive/data/outside/pubs/wordFrequency/google-ngrams/fiction/top100k.tab  | lstOp remove stdin /hive/data/outside/pubs/wordFrequency/bnc/bnc.txt > %s" % (rawWordListFname, wordListFname))

    if "tmatrix" in steps:
        runner = pubGeneric.makeClusterRunner(__file__, "pubClassify-tMatrix")
        pmids, dbs, dbPmids = parsePmids(pmidListDir)
        pubExpMatrix.runMatrixJobs(tMatrixFname, textDirs, wordListFname, None, None, \
                options.skipMap, "pmidsvml", options.test, posPmids=pmids, negPmids=[])
        logging.info("output matrix written to %s" % tMatrixFname)
        splitSvml(tMatrixFname, dbs, dbPmids, svmlDir)

    if "train" in steps:
        svmlLearn(svmlBinDir, svmlDir, modelDir, alphaDir, dbList)

    if "dmatrix" in steps:
        assert(len(textDirs)==1)
        pubExpMatrix.runMatrixJobs(dMatrixFname, textDirs, wordListFname, None, None, \
                options.skipMap, "svml", options.test)

    if "classify" in steps:
        #svmlClassify(svmlBinDir, dMatrixFname, modelDir, classDir, dbList)
        assert(len(textDirs)==1)
        textDir = textDirs[0]
        convertSvmlResults(dIdFname, classDir, categoryFname, dbList)

    if "html" in steps:
        makeHtml(categoryFname, pubConf.classOutHtmlDir, dbList)
        

# ----------- MAIN --------------
if args==[]:
    parser.print_help()
    exit(1)

main(args, options)
