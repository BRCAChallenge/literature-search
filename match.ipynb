{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Mentions to Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import sqlite3\n",
    "import functools\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mYou are using pip version 18.1, however version 19.0.2 is available.\r\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "os.environ['UTA_DB_URL']='postgresql://anonymous@rcurrie-uta:5432/uta/uta_20170117'\n",
    "os.environ['HGVS_SEQREPO_DIR']='/home/jovyan/data/pubmunch/references/seqrepo/2018-10-03'\n",
    "\n",
    "!pip install --user --quiet hgvs\n",
    "import hgvs.parser\n",
    "import hgvs.dataproviders.uta\n",
    "import hgvs.assemblymapper\n",
    "\n",
    "parser = hgvs.parser.Parser()\n",
    "server = hgvs.dataproviders.uta.connect()\n",
    "mapper = hgvs.assemblymapper.AssemblyMapper(server, assembly_name=\"GRCh38\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HGVS Mapping\n",
    "Use the [HGVS](https://hgvs.readthedocs.io/en/stable/) library to parse and map using a local reference server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NC_000017.11:g.43094426C>T'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@functools.lru_cache(maxsize=None)\n",
    "def parse_and_map_hgvs(candidate):\n",
    "    assert candidate\n",
    "    # Normalize single deletions: NM_007300.3:c.1100C>None -> NM_007300.3:c.1100delC\n",
    "    candidate = re.sub(r\"(NM.*c\\.\\d*)([ATCGatcg]+)(>None)\", r\"\\1del\\2\", candidate)\n",
    "\n",
    "    # TODO: Normalize multiple deletions and delins\n",
    "    # TODO: Limit to only specific BRCA transcripts\n",
    "    # ex: 23199084\tNM_007294.3:c.2681AA>None|NM_007300.3:c.2681AA>None\n",
    "\n",
    "    try:\n",
    "        parsed_hgvs = parser.parse_hgvs_variant(candidate)\n",
    "        try:\n",
    "            return str(mapper.c_to_g(parsed_hgvs))\n",
    "        except hgvs.exceptions.HGVSInvalidVariantError:\n",
    "            print(\"Failed Mapping: (HGVSInvalidVariantError): {}\".format(candidate))\n",
    "        except hgvs.exceptions.HGVSInvalidIntervalError:\n",
    "            print(\"Failed Mapping (HGVSInvalidIntervalError): {}\".format(candidate))\n",
    "        except hgvs.exceptions.HGVSDataNotAvailableError: \n",
    "            print(\"Failed Mapping (HGVSDataNotAvailableError): {}\".format(candidate))\n",
    "\n",
    "    except hgvs.exceptions.HGVSParseError:\n",
    "        print(\"Failed to parse: {}\".format(candidate))\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "parse_and_map_hgvs(\"NM_007294.3:c.1105G>A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to the crawl we want to match and export from\n",
    "# os.chdir(os.path.expanduser(\"/home/jovyan/data/pubmunch/crawl-2018-11-14/\"))\n",
    "os.chdir(os.path.expanduser(\"/home/jovyan/data/pubmunch/crawl/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize BRCA Exchange Variants\n",
    "\n",
    "Parse and map to genomic HGVS all of the variants tracked in the latest BRCA Exchange database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 21695 variants\n"
     ]
    }
   ],
   "source": [
    "variants = pd.read_csv(\"output/release/built_with_change_types.tsv\",\n",
    "                       sep=\"\\t\", header=0,\n",
    "                       usecols=[\"pyhgvs_Genomic_Coordinate_38\", \"pyhgvs_cDNA\", \"Synonyms\"])\n",
    "print(\"Loaded {} variants\".format(variants.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "variants = variants.sort_values(\"pyhgvs_cDNA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-1017T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-1027C>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-1122T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-1193C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-251G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-268C>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-280delG\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-296C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-357T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-395C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-407G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-418A>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-481G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-495G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-521delG\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-592G>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-630G>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-672C>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-700T>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-764A>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-845delA\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-869G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_000059.3:c.-924C>A\n",
      "Failed to parse: NM_000059.3:c.276_317-722del5070ins4\n",
      "Failed Mapping: (HGVSInvalidVariantError): NM_000059.3:c.595_6539del14318\n",
      "Failed Mapping: (HGVSInvalidVariantError): NM_000059.3:c.6418_7419del14500\n",
      "Failed Mapping: (HGVSInvalidVariantError): NM_000059.3:c.7397T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*1922T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*1984C>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*2602C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*2670A>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*2810A>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*3785T>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*4056C>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*4271T>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*4549G>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*5513G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*5677_*5678dupAT\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*5860C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.*6207C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-1044C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-1074C>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-1109C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-1457G>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-1465G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-1619G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-1648T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-1959C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-2030T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-2261T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-2266G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-2293C>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-2502T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-2569G>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-2614T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-2617_-2616delGT\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-264T>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-273G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-287C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-315delT\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-31665_81-4067del37071\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-33026_80+3829del38090\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-366C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-371G>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-378C>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-380G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-408T>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-497G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-516A>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-577A>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-585C>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-589A>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-593T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-676T>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-728A>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-730C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-848A>G\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-932T>C\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-939C>T\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-946G>A\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.-948G>A\n",
      "Failed Mapping: (HGVSInvalidVariantError): NM_007294.3:c.133_136del9196\n",
      "Failed to parse: NM_007294.3:c.1878_2126del249ins2\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.4358-1513_*37600del70050\n",
      "Failed to parse: NM_007294.3:c.442-952_547dup1058\n",
      "Failed Mapping: (HGVSInvalidVariantError): NM_007294.3:c.4987_5277del10644\n",
      "Failed to parse: NM_007294.3:c.5152+149_5193+2200del2593ins12\n",
      "Failed to parse: NM_007294.3:c.5213_5278-2753del3247ins1\n",
      "Failed Mapping (HGVSInvalidIntervalError): NM_007294.3:c.5406+664_*8273del11053\n",
      "Failed to parse: NM_007294.3:c.5468-285_*4016del4426ins2\n",
      "CPU times: user 1min 37s, sys: 5.74 s, total: 1min 43s\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "variants[\"norm_g_hgvs\"] = variants[\"pyhgvs_cDNA\"].apply(parse_and_map_hgvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load output from match.py\n",
    "# variants = pd.read_csv(\"variants-normalized.tsv\",\n",
    "#                        sep=\"\\t\", header=0, index_col=\"norm_g_hgvs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize and Match Mentions to Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "271\n",
      "271\n"
     ]
    }
   ],
   "source": [
    "# Wrangle mentions\n",
    "mentions = pd.read_csv(\"mutations-trimmed.tsv\", sep=\"\\t\", header=0, dtype=\"str\")\n",
    "print(mentions.shape[0])\n",
    "mentions = mentions.fillna(\"\")\n",
    "print(mentions.shape[0])\n",
    "mentions = mentions[mentions.mutSnippets != \"\"]\n",
    "print(mentions.shape[0])\n",
    "mentions = mentions[(mentions.hgvsCoding != \"\") | (mentions.texts != \"\")]\n",
    "print(mentions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions.docId.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions = mentions.sort_values([\"docId\", \"hgvsCoding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to match: hgvsCoding=NM_000410.3:c.845G>A Mapped=NC_000006.12:g.26092913G>A Texts= C282Y|C282Y\n",
      "Failed to match: hgvsCoding=NM_001257195.1:c.4T>G Mapped=NC_000005.10:g.137698400A>C Texts=S2A\n",
      "Failed to match: hgvsCoding= Mapped= Texts= L2H\n",
      "Failed to match: hgvsCoding= Mapped= Texts=S2A\n",
      "Failed to match: hgvsCoding= Mapped= Texts=2294delG\n",
      "Failed to match: hgvsCoding= Mapped= Texts=C2457T\n",
      "Failed to match: hgvsCoding= Mapped= Texts=G1639T\n",
      "Failed to match: hgvsCoding= Mapped= Texts=G5255A\n",
      "Failed to match: hgvsCoding= Mapped= Texts=C4731T\n",
      "Failed to match: hgvsCoding= Mapped= Texts=2294delG\n",
      "Failed to match: hgvsCoding= Mapped= Texts= C2457T\n",
      "Failed to match: hgvsCoding= Mapped= Texts=G1639T\n",
      "Failed to match: hgvsCoding= Mapped= Texts=G5255A\n",
      "Failed to match: hgvsCoding= Mapped= Texts=C4731T\n",
      "Failed to match: hgvsCoding= Mapped= Texts=/5382insC\n",
      "Failed to match: hgvsCoding= Mapped= Texts=S1C\n",
      "Failed to match: hgvsCoding= Mapped= Texts=S1C\n",
      "Failed to match: hgvsCoding= Mapped= Texts=I92P\n",
      "Failed to match: hgvsCoding=NM_001126112.2:c.2288T>None Mapped= Texts=2288delT\n",
      "Failed to match: hgvsCoding= Mapped= Texts=D17S\n",
      "Failed to match: hgvsCoding=NM_001126112.2:c.2288T>None Mapped= Texts=2288delT| 2288delT\n",
      "Failed to match: hgvsCoding= Mapped= Texts= E23f\n",
      "Failed to match: hgvsCoding= Mapped= Texts= E23f\n",
      "Failed to match: hgvsCoding= Mapped= Texts= Q1756f\n",
      "Failed to match: hgvsCoding= Mapped= Texts= N723f\n",
      "Failed to match: hgvsCoding= Mapped= Texts= D17S|D17S\n",
      "Failed to match: hgvsCoding= Mapped= Texts=I2*\n",
      "Failed to match: hgvsCoding= Mapped= Texts= I1*\n",
      "Failed to match: hgvsCoding= Mapped= Texts=I2*\n",
      "Failed to match: hgvsCoding= Mapped= Texts=I1*\n",
      "Failed to match: hgvsCoding=NM_007294.3:c.5558A>C Mapped=NC_000017.11:g.43045712T>G Texts=Y1853S\n",
      "Failed to match: hgvsCoding= Mapped= Texts=Q1756I\n",
      "Failed to match: hgvsCoding=NM_007294.3:c.5558A>C Mapped=NC_000017.11:g.43045712T>G Texts= Y1853S\n",
      "Failed to match: hgvsCoding= Mapped= Texts= Q1756I|Q1756I\n",
      "Failed to match: hgvsCoding= Mapped= Texts=W1777S| W1777S\n"
     ]
    }
   ],
   "source": [
    "def next_mention():\n",
    "    for i, row in mentions.iterrows():\n",
    "        matched = False\n",
    "        norm_g_hgvs = \"\"\n",
    "    \n",
    "        for raw_hgvs in set([r.strip() for r in row.hgvsCoding.split(\"|\")]):           \n",
    "\n",
    "            if not raw_hgvs:\n",
    "                continue\n",
    "                \n",
    "            norm_g_hgvs = parse_and_map_hgvs(raw_hgvs)\n",
    "            \n",
    "            if not norm_g_hgvs:\n",
    "                continue\n",
    "            \n",
    "            # normalized to normalized\n",
    "            if norm_g_hgvs in variants:\n",
    "                matched = True\n",
    "                yield (variants[norm_g_hgvs].pyhgvs_Genomic_Coordinate_38, row.docId, row.mutSnippets, 1)\n",
    "\n",
    "            # normalized to synonym (BRCA Exchange synonyms replace : with .)\n",
    "            for i, hit in variants.loc[\n",
    "                variants.Synonyms.str.contains(norm_g_hgvs.replace(\":\", \".\"), regex=False)].iterrows():\n",
    "                matched = True\n",
    "                yield (hit.pyhgvs_Genomic_Coordinate_38, row.docId, row.mutSnippets, 2)\n",
    "\n",
    "        # texts to synonym\n",
    "        # Note: Could always run but adds 40+ per variant...so only run if nothing else works\n",
    "        if not matched:\n",
    "            for text in set([t.strip() for t in row.texts.split(\"|\")]):\n",
    "                for i, hit in variants[variants.Synonyms.str.contains(text, regex=False)].iterrows():\n",
    "#                 for i, hit in itertools.islice(\n",
    "#                     variants[variants.Synonyms.str.contains(text, regex=False)].iterrows(), 10):\n",
    "                    matched = True\n",
    "                    yield (hit.pyhgvs_Genomic_Coordinate_38, row.docId, row.mutSnippets, 3)\n",
    "\n",
    "        if not matched:\n",
    "            print(\"Failed to match: hgvsCoding={} Mapped={} Texts={}\".format(\n",
    "                raw_hgvs, norm_g_hgvs, row.texts))    \n",
    "                \n",
    "\n",
    "# For each mention try to parse and normalize the hgvs\n",
    "matches = pd.DataFrame(\n",
    "    [m for m in next_mention()],\n",
    "    columns=[\"pyhgvs_Genomic_Coordinate_38\", \"pmid\", \"snippets\", \"score\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initial # Matches\", matches.shape[0])\n",
    "pruned_matches = matches.drop_duplicates([\"pyhgvs_Genomic_Coordinate_38\", \"pmid\", \"snippets\"])\n",
    "print(\"After dropping duplicates of pyhgvs_Genomic_Coordinate_38+pmid+snippets: {}\".format(pruned_matches.shape[0]))\n",
    "# print(\"Total unique genomic hgvs variants: {}\".format(mentions.index.unique().shape[0]))\n",
    "# mentions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some of the snippets are multiple mentions separated by | so unpack these,\n",
    "# but limit to 3 as some have as many as 168!\n",
    "print(\"Unpacking {} of {} snippets with multiple phrases seaparated by '|'\".format(\n",
    "    pruned_matches[pruned_matches.snippets.str.contains(\"|\", regex=False)].shape[0], pruned_matches.shape[0]))\n",
    "\n",
    "# https://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-rows/21032532\n",
    "\n",
    "# Reset index so each row id is unique vs. norm_g_hgvs\n",
    "df = pruned_matches.reset_index()\n",
    "# df = variant_mentions[variant_mentions.snippet.str.contains(\"|\", regex=False)].iloc[0:100].reset_index()\n",
    "\n",
    "# Generate a new dataframe splitting each snippet segment into its own row\n",
    "# Limit to max of 3 as some of them have > 100\n",
    "snippets = df.apply(lambda x: pd.Series(x.snippets.split(\"|\")[:3]), axis=1).stack()\n",
    "\n",
    "# To line up with the original index\n",
    "snippets.index = snippets.index.droplevel(-1)\n",
    "\n",
    "# Join back to the original dataframe replaceing the old \"snippet\" columnd\n",
    "snippets.name = \"snippets\"\n",
    "del df[\"snippets\"]\n",
    "exploded = df.join(snippets).drop_duplicates([\"pyhgvs_Genomic_Coordinate_38\", \"snippets\"]).set_index(\"pyhgvs_Genomic_Coordinate_38\")\n",
    "print(\"{} individual snippets after expanding and de-duplicating snippets\".format(exploded.shape[0]))\n",
    "\n",
    "exploded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(\"file:text/articles.db?mode=ro\", uri=True)\n",
    "articles = pd.read_sql_query(\"SELECT * FROM articles\", connection)\n",
    "articles.pmid = articles.pmid.astype(str)\n",
    "print(\"{} articles loaded from the articles sqlite database\".format(articles.shape[0]))\n",
    "articles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variants by pyhgvs_Genomic_Coordinate_38 by pmid with all snippets in a list\n",
    "combined = exploded.groupby([\"pyhgvs_Genomic_Coordinate_38\", \"pmid\"])[\"snippets\"].apply(lambda s: s.tolist())\n",
    "print(\"Combined {} separate snippets down to {} after grouping by pmid\".format(exploded.shape[0], combined.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "literature = {\n",
    "    \"date\": open(\"date.txt\").read().strip(),\n",
    "    \"papers\": articles[articles.pmid.isin(matches.pmid)].set_index(\"pmid\", drop=False).to_dict(orient=\"index\"),\n",
    "    \"variants\": {\n",
    "        k: {kk: vv[0] for kk, vv in v.unstack().transpose().iterrows()}\n",
    "        for k, v in combined.groupby(\"pyhgvs_Genomic_Coordinate_38\")},\n",
    "}\n",
    "\n",
    "with open(\"literature.json\", \"w\") as output:\n",
    "    output.write(json.dumps(literature, sort_keys=True))\n",
    "    \n",
    "print(\"Exported {} variants in {} papers\".format(\n",
    "    len(literature[\"variants\"].keys()), len(literature[\"papers\"].keys())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
