#!/usr/bin/env python2.7

# load default python packages
import logging, optparse, sys, os, traceback
from os.path import *

# add <scriptDir>/lib/ to package search path
sys.path.insert(0, join(dirname(abspath(__file__)), "lib"))

import pubGeneric, pubCrawl, pubConf, maxCommon

def main(args, options):
    outDir = args[0]
    if options.report:
        pubCrawl.writeReport(outDir, options.report)
        sys.exit(0)

    logFname = join(outDir, "crawler.log")
    pubGeneric.setupLogging("", options, logFileName=logFname, fileMode="a")

    try:
        pubCrawl.crawlFilesViaPubmed(outDir, options.waitTime, options.testPmid, \
            options.pause, options.tryHard, options.restrictPublisher)
    except KeyboardInterrupt:
        logging.info("stopped with ctrl-c, no email sent")
        raise
    except:
        if options.sendEmail:
            logging.info("Exception thrown during crawl. Sending error email to %s" % pubConf.email)
            subject = 'pubCrawl %s error' % outDir
            tb = traceback.format_exc()
            text = tb
            maxCommon.sendEmail(pubConf.email, subject, text)
        print "Unexpected error:", sys.exc_info()[0]
        raise

    if options.sendEmail:
        maxCommon.sendEmail(pubConf.email, outDir+" finished", "crawling finished")

# === COMMAND LINE INTERFACE, OPTIONS AND HELP ===
parser = optparse.OptionParser("""usage: %prog [options] <outDir> - crawl articles with supp files from websites. Looks for pmids.txt in outDir.""")

parser.add_option("-r", "--restrictPublisher", dest="restrictPublisher", action="store_true", help="uses the name of the output directory to find webservers for this publisher and limits downloads to the webservers defined in pubConf.py")
parser.add_option("-d", "--debug", dest="debug", action="store_true", help="show debug messages")
parser.add_option("-v", "--verbose", dest="verbose", action="store_true", help="show more debug messages")
parser.add_option("-t", "--waitTime", dest="waitTime", action="store", type="int", help="number of seconds to wait between http requests, default %default", default=10)
parser.add_option("", "--test", dest="testPmid", action="store", help="test crawling with this PMID")
parser.add_option("-p", "--pause", dest="pause", action="store_true", help="wait for keypress after each download")
parser.add_option("-e", "--sendEmail", dest="sendEmail", action="store_true", help="send an error email to address specified in pubConf when program crashes")
parser.add_option("", "--tryHarder", dest="tryHard", action="store_true", help="the default behaviour is to back off when errors occur, i.e. make the delays between requests longer and longer (60secs*number of errors) until the first download was successful again. This is to allow for repairs of servers, e.g. during the night. This option changes the wait time and will always wait for 5 secs, for downloads where we expect many errors. It also increases tolerance for errors, will stop crawling only after 500 consecutive errors")
parser.add_option("", "--report", dest="report", action="store", help="Do not crawl. given the base crawl directory, write a status report in html format to the specified file and quite. ")
(options, args) = parser.parse_args()

if args==[]:
    parser.print_help()
    exit(1)

main(args, options)
