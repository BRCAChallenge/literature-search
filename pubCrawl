#!/usr/bin/env python

# load default python packages
import logging, optparse, sys, os, traceback
from os.path import *

# add <scriptDir>/lib/ to package search path
sys.path.insert(0, join(dirname(abspath(__file__)), "lib"))

import pubGeneric, pubCrawl, pubConf, maxCommon, pubCrawlConf

def main(args, options):
    outDirs = args
    if options.report:
        pubCrawl.writeReport(outDirs[0], options.report)
        sys.exit(0)

    pubCrawlConf.initConfig()

    try:
        for outDir in outDirs:
            logFname = join(outDir, "crawler.log")
            if options.testPmid:
                pubGeneric.setupLogging("", options)
            else:
                pubGeneric.setupLogging("", options, logFileName=logFname, fileMode="a")

            if options.waitTime:
                pubCrawl.globalForceDelay = options.waitTime

            pubCrawl.crawlFilesViaPubmed(outDir, options.testPmid, \
                options.pause, options.tryHard, options.restrictPublisher, options.localMedline, \
                options.fakeUseragent, options.preferPmc, options.skipDb, options.publisher)
    except KeyboardInterrupt:
        logging.info("stopped with ctrl-c, no email sent")
        raise
    except:
        if options.sendEmail:
            logging.info("Exception thrown during crawl. Sending error email to %s" % pubConf.email)
            subject = 'pubCrawl %s error' % outDir
            tb = traceback.format_exc()
            text = tb
            maxCommon.sendEmail(pubConf.email, subject, text)
        print "Unexpected error:", sys.exc_info()[0]
        raise

    if options.sendEmail:
        maxCommon.sendEmail(pubConf.email, outDir+" finished", "crawling finished")

# === COMMAND LINE INTERFACE, OPTIONS AND HELP ===
parser = optparse.OptionParser("""usage: %prog [options] <outDir1> <outDir2> ... - crawl articles with supp files from websites of one or more publishers. Reads files named pmids.txt in outDir.""")

parser.add_option("-r", "--restrictPublisher", dest="restrictPublisher", action="store_true", help="uses the name of the output directory to find webservers for this publisher and limits downloads to the webservers defined in pubConf.py")
parser.add_option("-d", "--debug", dest="debug", action="store_true", help="show debug messages")
parser.add_option("-v", "--verbose", dest="verbose", action="store_true", help="show more debug messages")
parser.add_option("-t", "--waitTime", dest="waitTime", action="store", type="int", help="number of seconds to wait between http requests, overrides all other settings")
parser.add_option("", "--test", dest="testPmid", action="store", help="test crawling with this PMID")
parser.add_option("-p", "--pause", dest="pause", action="store_true", help="wait for keypress after each download")
parser.add_option("-e", "--sendEmail", dest="sendEmail", action="store_true", help="send an error email to address specified in pubConf when program crashes")
parser.add_option("-l", "--localMedline", dest="localMedline", action="store_true", help="the crawler can get MedLine info from either a local sqlite copy or use NCBI's eutils. This parameter activates local medline lookups.")
parser.add_option("-u", "--fakeUseragent", dest="fakeUseragent", action="store_true", help="by default, the crawler accounces itself to the publisher's webserver as 'genomeBot/0.1'. This parameter changes the behaviour and the crawler will present itself as Firefox. Use this with caution and only if the publisher/hoster accepts it.")
parser.add_option("", "--preferPmc", dest="preferPmc", action="store_true", help="by default, the crawler will go to the original publisher's site if a paper is on PMC and somewhere else. This option will always go to PMC if possible. Note that PMC does not allow crawling so use at your own risk.")
parser.add_option("", "--tryHarder", dest="tryHard", action="store_true", help="the default behaviour is to back off when errors occur, i.e. make the delays between requests longer and longer (60secs*number of errors) until the first download was successful again. This is to allow for repairs of servers, e.g. during the night. This option changes the wait time and will always wait for 5 secs, for downloads where we expect many errors. It also increases tolerance for errors, will stop crawling only after 500 consecutive errors")
parser.add_option("", "--report", dest="report", action="store", help="Do not crawl. given the base crawl directory, write a status report in html format to the specified first output filename and quit. ")
parser.add_option("-s", "--skipDb", dest="skipDb", action="store_true", help="Usually pubCrawl will parse both the pmidStatus.tab and the sqlite db to figure out which PMIDs have been done before. This option will not read the sqlite db. This will improve startup time for large publishers like Wiley.")
parser.add_option("", "--publisher", dest="publisher", action="store", help="name of publisher. By default, this is the name of the subdirectory where the pmids.txt file is located. This option can override the publisher name to something else defined in pubCrawlConf.py")
(options, args) = parser.parse_args()

if args==[]:
    parser.print_help()
    exit(1)

main(args, options)
