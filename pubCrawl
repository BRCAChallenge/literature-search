#!/usr/bin/env python

# load default python packages
import logging, optparse, sys, os, traceback
from os.path import *

# add <scriptDir>/lib/ to package search path
sys.path.insert(0, join(dirname(abspath(__file__)), "lib"))

import pubGeneric, pubCrawl, pubConf, maxCommon

def main(args, options):
    outDir = args[0]
    if options.report:
        pubCrawl.writeReport(outDir, options.report)
        sys.exit(0)

    logFname = join(outDir, "crawler.log")
    pubGeneric.setupLogging("", options, logFileName=logFname, fileMode="a")

    try:
        pubCrawl.crawlFilesViaPubmed(outDir, options.waitTime, options.testPmid, \
            options.pause, options.tryHard, options.restrictPublisher, options.localMedline, \
            options.fakeUseragent, options.preferPmc)
    except KeyboardInterrupt:
        logging.info("stopped with ctrl-c, no email sent")
        raise
    except:
        if options.sendEmail:
            logging.info("Exception thrown during crawl. Sending error email to %s" % pubConf.email)
            subject = 'pubCrawl %s error' % outDir
            tb = traceback.format_exc()
            text = tb
            maxCommon.sendEmail(pubConf.email, subject, text)
        print "Unexpected error:", sys.exc_info()[0]
        raise

    if options.sendEmail:
        maxCommon.sendEmail(pubConf.email, outDir+" finished", "crawling finished")

# === COMMAND LINE INTERFACE, OPTIONS AND HELP ===
parser = optparse.OptionParser("""usage: %prog [options] <outDir> - crawl articles with supp files from websites. Looks for pmids.txt in outDir.""")

parser.add_option("-r", "--restrictPublisher", dest="restrictPublisher", action="store_true", help="uses the name of the output directory to find webservers for this publisher and limits downloads to the webservers defined in pubConf.py")
parser.add_option("-d", "--debug", dest="debug", action="store_true", help="show debug messages")
parser.add_option("-v", "--verbose", dest="verbose", action="store_true", help="show more debug messages")
parser.add_option("-t", "--waitTime", dest="waitTime", action="store", type="int", help="number of seconds to wait between http requests, default %default", default=10)
parser.add_option("", "--test", dest="testPmid", action="store", help="test crawling with this PMID")
parser.add_option("-p", "--pause", dest="pause", action="store_true", help="wait for keypress after each download")
parser.add_option("-e", "--sendEmail", dest="sendEmail", action="store_true", help="send an error email to address specified in pubConf when program crashes")
parser.add_option("-l", "--localMedline", dest="localMedline", action="store_true", help="the crawler can get MedLine info from either a local sqlite copy or use NCBI's eutils. This parameter activates local medline lookups.")
parser.add_option("-u", "--fakeUseragent", dest="fakeUseragent", action="store_true", help="by default, the crawler accounces itself to the publisher's webserver as 'genomeBot/0.1'. This parameter changes the behaviour and the crawler will present itself as Firefox. Use this with caution and only if the publisher/hoster accepts it.")
parser.add_option("", "--preferPmc", dest="preferPmc", action="store_true", help="by default, the crawler will go to the original publisher's site if a paper is on PMC and somewhere else. This option will always go to PMC if possible. Note that PMC does not allow crawling so use at your own risk.")
parser.add_option("", "--tryHarder", dest="tryHard", action="store_true", help="the default behaviour is to back off when errors occur, i.e. make the delays between requests longer and longer (60secs*number of errors) until the first download was successful again. This is to allow for repairs of servers, e.g. during the night. This option changes the wait time and will always wait for 5 secs, for downloads where we expect many errors. It also increases tolerance for errors, will stop crawling only after 500 consecutive errors")
parser.add_option("", "--report", dest="report", action="store", help="Do not crawl. given the base crawl directory, write a status report in html format to the specified file and quite. ")
(options, args) = parser.parse_args()

if args==[]:
    parser.print_help()
    exit(1)

main(args, options)
